{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "WebApp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StePetro/ComputerVisionProject/blob/main/WebApp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmACb54E-uGg"
      },
      "source": [
        "#SETUP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lc0X7O2689Pa",
        "outputId": "36102e3e-d3f9-482b-942a-d5b9d396c6ad"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQwsrVZf9Dcz",
        "outputId": "dcc5fc2f-a62e-4d79-bb23-8a9ded0eb30e"
      },
      "source": [
        "!apt-get install -qq xattr\n",
        "!pip install scikit-learn-extra\n",
        "!pip install flask-ngrok"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn-extra in /usr/local/lib/python3.6/dist-packages (0.1.0b2)\n",
            "Requirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn-extra) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn-extra) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn-extra) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.0->scikit-learn-extra) (1.0.0)\n",
            "Requirement already satisfied: flask-ngrok in /usr/local/lib/python3.6/dist-packages (0.0.25)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (1.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.2)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2020.12.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask-ngrok) (1.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFDo5VYk9Lro",
        "outputId": "15188629-6829-40d1-8d8b-0cc6def6f9f2"
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "import numpy as np\n",
        "import glob\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "import tensorflow.keras.backend as KK\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from IPython.display import display\n",
        "from keras.regularizers import l2\n",
        "from matplotlib.pyplot import imshow\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from PIL import Image, ImageDraw, ImageOps\n",
        "from IPython.display import display\n",
        "from time import sleep\n",
        "import sklearn\n",
        "import copy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.preprocessing import normalize, minmax_scale\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask,render_template,request, abort\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.python.data.ops import dataset_ops\n",
        "from tensorflow.python.keras.layers.preprocessing import image_preprocessing\n",
        "from tensorflow.python.keras.preprocessing import dataset_utils\n",
        "from tensorflow.python.ops import image_ops\n",
        "from tensorflow.python.ops import io_ops\n",
        "from tensorflow.python.util.tf_export import keras_export\n",
        "import xattr\n",
        "\n",
        "import os\n",
        "import datetime\n",
        "from numpy import genfromtxt\n",
        "from csv import reader\n",
        "import pandas as pd\n",
        "import bisect\n",
        "from numpy import save\n",
        "import scipy as sc\n",
        "from tqdm import tqdm\n",
        "import heapq\n",
        "import random\n",
        "import itertools as itool\n",
        "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
        "\n",
        "DATA_PATH = '/content/gdrive/My Drive/Data/'\n",
        "MODELS_PATH = DATA_PATH + 'models'\n",
        "FACIAL_EXPRESSION_PATH = '/content/dataset'\n",
        "MIRFLICKR_PATH = \"/content/distractor\"\n",
        "FEATURES_PATH = DATA_PATH + \"features\"\n",
        "CATEGORICAL_LABELS = {0:\"Angry\", 1:\"Disgust\", 2:\"Fear\", 3:\"Happy\", 4:\"Neutral\", 5:\"Sad\", 6:\"Surprise\", 99:\"Distractor\"}\n",
        "FACIAL_EXPRESSION_SETS = [\"Training\", \"PrivateTest\", \"PublicTest\"]\n",
        "WHITELIST_FORMATS = ('.bmp', '.gif', '.jpeg', '.jpg', '.png')\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = BATCH_SIZE\n",
        "VALIDATION_SPLIT = 0.2\n",
        "SEED = 76\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKOwG2mE-Ax8"
      },
      "source": [
        "\"\"\"\n",
        "IMAGE_DATASET_FROM_DIRECTORY FUNCTION OVERRIDE\n",
        "\"\"\"\n",
        "\n",
        "def image_dataset_from_directory(directory,\n",
        "                                 labels='inferred',\n",
        "                                 label_mode='int',\n",
        "                                 class_names=None,\n",
        "                                 color_mode='rgb',\n",
        "                                 batch_size=32,\n",
        "                                 image_size=(256, 256),\n",
        "                                 shuffle=True,\n",
        "                                 seed=None,\n",
        "                                 validation_split=None,\n",
        "                                 subset=None,\n",
        "                                 interpolation='bilinear',\n",
        "                                 follow_links=False):\n",
        "  \n",
        "  if labels != 'inferred':\n",
        "    if not isinstance(labels, (list, tuple)):\n",
        "      raise ValueError(\n",
        "          '`labels` argument should be a list/tuple of integer labels, of '\n",
        "          'the same size as the number of image files in the target '\n",
        "          'directory. If you wish to infer the labels from the subdirectory '\n",
        "          'names in the target directory, pass `labels=\"inferred\"`. '\n",
        "          'If you wish to get a dataset that only contains images '\n",
        "          '(no labels), pass `label_mode=None`.')\n",
        "    if class_names:\n",
        "      raise ValueError('You can only pass `class_names` if the labels are '\n",
        "                       'inferred from the subdirectory names in the target '\n",
        "                       'directory (`labels=\"inferred\"`).')\n",
        "  if label_mode not in {'int', 'categorical', 'binary', None}:\n",
        "    raise ValueError(\n",
        "        '`label_mode` argument must be one of \"int\", \"categorical\", \"binary\", '\n",
        "        'or None. Received: %s' % (label_mode,))\n",
        "  if color_mode == 'rgb':\n",
        "    num_channels = 3\n",
        "  elif color_mode == 'rgba':\n",
        "    num_channels = 4\n",
        "  elif color_mode == 'grayscale':\n",
        "    num_channels = 1\n",
        "  else:\n",
        "    raise ValueError(\n",
        "        '`color_mode` must be one of {\"rbg\", \"rgba\", \"grayscale\"}. '\n",
        "        'Received: %s' % (color_mode,))\n",
        "  interpolation = image_preprocessing.get_interpolation(interpolation)\n",
        "  dataset_utils.check_validation_split_arg(\n",
        "      validation_split, subset, shuffle, seed)\n",
        "\n",
        "  if seed is None:\n",
        "    seed = np.random.randint(1e6)\n",
        "  image_paths, labels, class_names = dataset_utils.index_directory(\n",
        "      directory,\n",
        "      labels,\n",
        "      formats=WHITELIST_FORMATS,\n",
        "      class_names=class_names,\n",
        "      shuffle=shuffle,\n",
        "      seed=seed,\n",
        "      follow_links=follow_links)\n",
        "\n",
        "  if label_mode == 'binary' and len(class_names) != 2:\n",
        "    raise ValueError(\n",
        "        'When passing `label_mode=\"binary\", there must exactly 2 classes. '\n",
        "        'Found the following classes: %s' % (class_names,))\n",
        "\n",
        "  image_paths, labels = dataset_utils.get_training_or_validation_split(\n",
        "      image_paths, labels, validation_split, subset)\n",
        "\n",
        "  dataset = paths_and_labels_to_dataset(\n",
        "      image_paths=image_paths,\n",
        "      image_size=image_size,\n",
        "      num_channels=num_channels,\n",
        "      labels=labels,\n",
        "      label_mode=label_mode,\n",
        "      num_classes=len(class_names),\n",
        "      interpolation=interpolation)\n",
        "  if shuffle:\n",
        "    # Shuffle locally at each iteration\n",
        "    dataset = dataset.shuffle(buffer_size=batch_size * 8, seed=seed)\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  # Users may need to reference `class_names`.\n",
        "  dataset.class_names = class_names\n",
        "  return dataset, image_paths\n",
        "\n",
        "def paths_and_labels_to_dataset(image_paths,\n",
        "                                image_size,\n",
        "                                num_channels,\n",
        "                                labels,\n",
        "                                label_mode,\n",
        "                                num_classes,\n",
        "                                interpolation):\n",
        "  \"\"\"Constructs a dataset of images and labels.\"\"\"\n",
        "  # TODO(fchollet): consider making num_parallel_calls settable\n",
        "  path_ds = dataset_ops.Dataset.from_tensor_slices(image_paths)\n",
        "  img_ds = path_ds.map(\n",
        "      lambda x: path_to_image(x, image_size, num_channels, interpolation))\n",
        "  if label_mode:\n",
        "    label_ds = dataset_utils.labels_to_dataset(labels, label_mode, num_classes)\n",
        "    img_ds = dataset_ops.Dataset.zip((img_ds, label_ds))\n",
        "  return img_ds\n",
        "\n",
        "\n",
        "def path_to_image(path, image_size, num_channels, interpolation):\n",
        "  img = io_ops.read_file(path)\n",
        "  img = image_ops.decode_image(\n",
        "      img, channels=num_channels, expand_animations=False)\n",
        "  img = image_ops.resize_images_v2(img, image_size, method=interpolation)\n",
        "  img.set_shape((image_size[0], image_size[1], num_channels))\n",
        "  return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhC3WmSH9K0o"
      },
      "source": [
        "#STEP 1: PP-INDEX IMPLEMENTATION\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHIyg_pb9K0x"
      },
      "source": [
        "##Permutations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWw8d3bn9K0y"
      },
      "source": [
        "###Pivots Choice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p05zsibm9K0z"
      },
      "source": [
        "def extract_random_pivots(input_file, pivots_file, N):\n",
        "  \"\"\"\n",
        "  Extract N random pivots from input_file and save them in output_file\n",
        "  \"\"\"\n",
        "  \n",
        "  #Extract N rows at random without opening the entire file\n",
        "  with open(os.path.join(FEATURES_PATH,input_file)) as fin:\n",
        "      sample = heapq.nlargest(N, fin, key=lambda L: random.random())\n",
        "\n",
        "  # Delete old pivots_file\n",
        "  if os.path.isfile(os.path.join(BASE_PP_INDEX_PATH,pivots_file)):\n",
        "   os.remove(os.path.join(BASE_PP_INDEX_PATH,pivots_file))\n",
        "\n",
        "  #Save new pivots_file\n",
        "  with open(os.path.join(BASE_PP_INDEX_PATH,pivots_file), 'w+') as piv:\n",
        "    for s in sample:\n",
        "       piv.write(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyNaJU8h9K00"
      },
      "source": [
        "def extract_k_medoids_pivots(input_file, pivots_file, aproximation_size, N):\n",
        "  \"\"\"\n",
        "  Extract N pivots from input_file unsing in memory k-medoids algorithm and save them in output_file\n",
        "  \"\"\"\n",
        "  \n",
        "  #Extract N rows at random without opening the entire file\n",
        "  with open(os.path.join(FEATURES_PATH,input_file)) as fin:\n",
        "      strings_sample = heapq.nlargest(aproximation_size, fin, key=lambda L: random.random())\n",
        "\n",
        "  # Parse to remove the string path from the samples\n",
        "  sample = np.array([np.float64(s.split(\",\")[1:]) for s in strings_sample])\n",
        "\n",
        "  # Take the indexes of the cluster centers\n",
        "  pivots_index = KMedoids(N,\"cosine\").fit(sample[:,1:]).medoid_indices_\n",
        "\n",
        "  # Use the clusters centers as pivots\n",
        "  pivots = sample[pivots_index]\n",
        "  pivots = np.column_stack([np.zeros(N).reshape(-1,1), pivots])\n",
        "\n",
        "  # Delete old pivots_file\n",
        "  if os.path.isfile(os.path.join(BASE_PP_INDEX_PATH,pivots_file)):\n",
        "    os.remove(os.path.join(BASE_PP_INDEX_PATH,pivots_file))\n",
        "\n",
        "  #Save new pivots_file\n",
        "  with open(os.path.join(BASE_PP_INDEX_PATH,pivots_file), 'w+') as piv:\n",
        "    np.savetxt(piv,pivots,delimiter=\",\")     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8TLyy9G9K00"
      },
      "source": [
        "###Permutation Computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNJh71xn9K01"
      },
      "source": [
        "def create_permutation_dataset(input_file, output_file, pivots_file, chunksize):\n",
        "  \"\"\"\n",
        "  Compute pivots permutation for each element and save it in a file\n",
        "  \"\"\"\n",
        "\n",
        "  #Take pivots\n",
        "  df_pivots=pd.read_csv(os.path.join(BASE_PP_INDEX_PATH,pivots_file), sep=',',header=None )\n",
        "  pivots = df_pivots.to_numpy()\n",
        "\n",
        "  #Take features\n",
        "  df_features=pd.read_csv(os.path.join(FEATURES_PATH,input_file), sep=',',header=None, chunksize = chunksize)\n",
        "\n",
        "  # Delete old permutation file\n",
        "  if os.path.isfile(os.path.join(BASE_PP_INDEX_PATH,output_file)):\n",
        "    os.remove(os.path.join(BASE_PP_INDEX_PATH,output_file))\n",
        "\n",
        "  #Compute permutations\n",
        "  for chunk in tqdm(df_features):\n",
        "    chunk = np.array(chunk)\n",
        "\n",
        "    distances_list_chunk = []\n",
        "    for elem in chunk: \n",
        "\n",
        "      distances_list = []\n",
        "\n",
        "      #Compute distance list between elem and pivots\n",
        "      for piv in pivots:\n",
        "\n",
        "        dist = cosine_distances(np.array(elem[1:]).reshape(1,-1),np.array(piv[1:]).reshape(1,-1))[0]\n",
        "        distances_list.extend(dist)\n",
        "        distances_from_pivots = np.array(distances_list)\n",
        "\n",
        "      # Reorder the distance list (to obtain the permutation)\n",
        "      distances_list_chunk.append(np.array(pd.DataFrame(np.argsort(distances_from_pivots)).T).reshape(-1))\n",
        "\n",
        "    # Save permutations to csv file \n",
        "    pd.DataFrame(distances_list_chunk).to_csv(os.path.join(BASE_PP_INDEX_PATH,output_file), mode='a', index = False, header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fD0rJRw9K01"
      },
      "source": [
        "def compute_permutation_prefix(query, dim_prefix, pivots):\n",
        "  \"\"\"\n",
        "  Compute prefix of a given query \n",
        "  \"\"\"\n",
        "\n",
        "  permutations_list=[]\n",
        "  distances_list = []\n",
        "\n",
        "  # Compute distance list between query and pivots \n",
        "  for piv in pivots:\n",
        "    dist = cosine_distances(np.array(query[1:]).reshape(1,-1),np.array(piv[1:]).reshape(1,-1))[0]\n",
        "    distances_list.extend(dist)\n",
        "  \n",
        "  # Compute permutation: reoder the distance list \n",
        "  distances_from_pivots = np.array(distances_list)\n",
        "  permutations_list.extend(np.argsort(distances_from_pivots))\n",
        "  # Return the prefix (=the first dim_prefix elements of permutation)\n",
        "  return np.array(permutations_list)[:dim_prefix]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqpEi6e69K02"
      },
      "source": [
        "##Prefix Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AN_my0s9K02"
      },
      "source": [
        "class PrefixTree(object):\n",
        "    \"\"\"\n",
        "    Prefix tree implementation\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, value: str, is_leaf = False, is_last_pivot = False):\n",
        "        # Value of the node\n",
        "        self.value = value\n",
        "        self.children = []\n",
        "        # If this is a leaf node\n",
        "        self.is_leaf = is_leaf\n",
        "        self.is_last_pivot = is_last_pivot\n",
        "        # How many images have this prefix\n",
        "        self.counter = 1\n",
        "    \n",
        "\n",
        "    def add(root, permutation, position_index):\n",
        "        \"\"\"\n",
        "        Adding a permutation in the prefix tree structure\n",
        "        \"\"\"\n",
        "        node = root\n",
        "        for idx, pivot in enumerate(permutation):\n",
        "            found_in_child = False\n",
        "            # Search for the pivot in the children of the present `node`\n",
        "            for child in node.children:\n",
        "                if child.value == pivot:\n",
        "                    # Increase the counter by 1 to keep track that another\n",
        "                    # permutation has hit\n",
        "                    child.counter += 1\n",
        "                    # Point node to the child that contains this pivot\n",
        "                    node = child\n",
        "                    found_in_child = True                    \n",
        "            # If pivot is not find add a new chlid\n",
        "            if not found_in_child:\n",
        "                new_node = PrefixTree(pivot)\n",
        "                node.children.append(new_node)\n",
        "                # Point node to the new child\n",
        "                node = new_node\n",
        "            if idx == (len(permutation) - 1):\n",
        "                #last\n",
        "                node.children.append(PrefixTree(position_index, is_leaf = True))\n",
        "            # Last prefix's pivot\n",
        "        node.is_last_pivot = True\n",
        "\n",
        "\n",
        "    def find_prefix(root, prefix: str, k):\n",
        "      \"\"\"\n",
        "      Find the prefix subtree with that prefix and at least k leaves\n",
        "      \"\"\"\n",
        "      node = root\n",
        "      # If the root node has no children, then return False.\n",
        "      if not root.children:\n",
        "        return False\n",
        "\n",
        "      for pivot in prefix:\n",
        "        pivot_not_found = True\n",
        "\n",
        "        # Search through all the children\n",
        "        for child in node.children:\n",
        "                  \n",
        "          if child.value == pivot:\n",
        "            pivot_not_found = False        \n",
        "            if(child.counter < k):\n",
        "              # If child leaves < k return parent to have at least k leaves\n",
        "              return node\n",
        "            node = child\n",
        "            break\n",
        "\n",
        "        # Return False anyway when not find a pivot\n",
        "        if pivot_not_found:\n",
        "          return False\n",
        "\n",
        "      return node\n",
        "\n",
        "    def get_leaves(node):\n",
        "      \"\"\"\n",
        "      Get prefix tree's leaves in order\n",
        "      \"\"\"\n",
        "      leaves = []\n",
        "      # Check each children of a node\n",
        "      for child in node.children:\n",
        "        if child.is_leaf:\n",
        "          # Take the leaf value\n",
        "          leaves.append(child.value)\n",
        "        else:\n",
        "          leaves.extend(PrefixTree.get_leaves(child))\n",
        "      return leaves\n",
        "\n",
        "    \n",
        "    def extract_parent_pointers(node):\n",
        "      \"\"\"\n",
        "      Extract leaves parents\n",
        "      \"\"\"\n",
        "      parent_pointers = []\n",
        "      # Check each children of a node\n",
        "      for child in node.children:\n",
        "        if child.is_last_pivot:  \n",
        "          # If is_last_pivot = True it means that the node children are leaves\n",
        "          parent_pointers.append(child) \n",
        "        if not child.is_last_pivot:\n",
        "          parent_pointers.extend(PrefixTree.extract_parent_pointers(child))\n",
        "      return parent_pointers\n",
        "\n",
        "\n",
        "    def merge_leaves(node):\n",
        "      \"\"\"\n",
        "      Merge pointers to reflect new order\n",
        "      \"\"\"\n",
        "      parent_pointers = PrefixTree.extract_parent_pointers(node)\n",
        "      #Use idx to save how many elements merged in the leaves \n",
        "      idx = 0\n",
        "      # For each leaf parent\n",
        "      for parent in parent_pointers:\n",
        "        # If there are more than one leaf\n",
        "        if len(parent.children)!= 1:\n",
        "          #Merge and save start-idx  and end-idx\n",
        "          size = len(parent.children)\n",
        "          parent.children = [ PrefixTree(idx, is_leaf = True),  \n",
        "                              PrefixTree(idx + len(parent.children) - 1, is_leaf = True) ]\n",
        "          idx += size\n",
        "        else:\n",
        "          parent.children = [ PrefixTree(idx, is_leaf = True)]  \n",
        "          idx += 1\n",
        "\n",
        " \n",
        "    def print_tree(node, _prefix=\"\", _last=True):\n",
        "      \"\"\"\n",
        "      Print prefix tree's structure\n",
        "      \"\"\"\n",
        "      print(_prefix, \"`- \" if _last else \"|- \", node.value, sep=\"\")\n",
        "      _prefix += \"   \" if _last else \"|  \"\n",
        "      child_count = len(node.children)\n",
        "      for i, child in enumerate(node.children):\n",
        "        _last = i == (child_count - 1)\n",
        "        PrefixTree.print_tree(child, _prefix, _last)\n",
        "\n",
        "  \n",
        "    def add_all(root, permutations, prefix_len, base_index = 0):\n",
        "      '''\n",
        "      Insert in the tree all the permutations cutting them according to prefix_len\n",
        "      '''\n",
        "      for index, prefix in enumerate(permutations):\n",
        "        PrefixTree.add(root, np.array(prefix).reshape(-1)[:prefix_len], base_index + index)\n",
        "\n",
        "\n",
        "    def add_from_csv(root, permutation_file, prefix_len, chunksize):\n",
        "      \"\"\"\n",
        "      Insert in the tree all the permutations cutting them according to prefix_len\n",
        "      from csv permutation_file in chunks of chunksize\n",
        "      \"\"\"\n",
        "      permutations_chunks = pd.read_csv(os.path.join(BASE_PP_INDEX_PATH,permutation_file), sep=',', header=None, chunksize = chunksize)\n",
        "      for i, permutations_chunk in tqdm(enumerate(permutations_chunks)):\n",
        "        PrefixTree.add_all(root, np.array(permutations_chunk), prefix_len, chunksize*i)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92Kmjzta9K03"
      },
      "source": [
        "##Reordering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYm-ofQS9K03"
      },
      "source": [
        "def reorder_datastore_in_memory(input_file, output_file, indexing_mapping):\n",
        "  \"\"\"\n",
        "  Reorder the rows of a csv file (in memory) according to indexing_mapping \n",
        "  \"\"\"\n",
        "  # Read file\n",
        "  features = pd.read_csv(os.path.join(FEATURES_PATH,input_file), sep=',', header=None)\n",
        "  # Insert a new column containing the Indexing\n",
        "  features.insert(0, 'Indexing', indexing_mapping)\n",
        "  # Sort the rows by the Indexing \n",
        "  features = features.sort_values(by=['Indexing']).drop(labels = 'Indexing', axis=1)  \n",
        "  # Save reordered rows in new file\n",
        "  features.to_csv(os.path.join(BASE_PP_INDEX_PATH,output_file), mode='w', index = False, header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUi-700X9K03"
      },
      "source": [
        "def reorder_datastore_on_disk(input_file, output_file, leaves_list):\n",
        "  \"\"\"\n",
        "  Reorder the rows of a csv file (on disk) according to leaves_list\n",
        "  \"\"\"\n",
        "  # Delete old file\n",
        "  if os.path.isfile(os.path.join(BASE_PP_INDEX_PATH,output_file)):\n",
        "    os.remove(os.path.join(BASE_PP_INDEX_PATH,output_file))\n",
        "\n",
        "  # For each leaf in the list\n",
        "  for leaf in tqdm(leaves_list):\n",
        "\n",
        "    # Read a row and save it to output_file (append mode)\n",
        "    df_features = pd.read_csv(os.path.join(FEATURES_PATH,input_file), sep=',', header=None, skiprows= leaf, nrows=1).to_csv(os.path.join(BASE_PP_INDEX_PATH,output_file), mode='a', index = False, header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4kAG--d9K04"
      },
      "source": [
        "##PP-Index Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNqVV3YL9K04"
      },
      "source": [
        "def create_PP_index(n_pivots, prefix_len, features_file, pivots_file, permutations_file, reordered_features_file, labels_file, reordered_labels_file, k_medoids = False, aproximation_size = 25000, chunksize = 1000, on_disk = False, num_indexes = 1):\n",
        "  '''\n",
        "  Creates the PP Index and returns the prefix tree\n",
        "  Pass only file name without extention\n",
        "  '''\n",
        "\n",
        "  prefix_trees = []\n",
        "\n",
        "  for idx in range(num_indexes):\n",
        "    # Pivot extraction from features space\n",
        "    if k_medoids:\n",
        "      extract_k_medoids_pivots(features_file + \".csv\",pivots_file + \"_\" + str(idx) + \".csv\", aproximation_size, n_pivots)\n",
        "    else:\n",
        "      extract_random_pivots(features_file + \".csv\", pivots_file + \"_\" + str(idx) + \".csv\", n_pivots)\n",
        "\n",
        "    # Dataset of images permutations of indexes \n",
        "    create_permutation_dataset(features_file + \".csv\", permutations_file + \"_\" + str(idx) + \".csv\", pivots_file + \"_\" + str(idx) + \".csv\", chunksize) \n",
        "\n",
        "    # Creates Empty Prefix Tree\n",
        "    prefix_tree_root = PrefixTree(\"*\")\n",
        "    # Adds all images pivots permutations in \"permutations.csv\"\n",
        "    PrefixTree.add_from_csv(prefix_tree_root, permutations_file + \"_\" + str(idx) + \".csv\", prefix_len, chunksize = chunksize)\n",
        "\n",
        "    # Get images indexes ordered by the prefix tree\n",
        "    indexes_reordered_by_prefix_tree = PrefixTree.get_leaves(prefix_tree_root)\n",
        "    print(indexes_reordered_by_prefix_tree)\n",
        "\n",
        "    if on_disk:\n",
        "      # On Disk features and labels reordering (Slow using colab and google drive)\n",
        "      reorder_datastore_on_disk(features_file + \".csv\", reordered_features_file + \"_\" + str(idx) + \".csv\", indexes_reordered_by_prefix_tree)\n",
        "      reorder_datastore_on_disk(labels_file + \".csv\", reordered_labels_file + \"_\" + str(idx) + \".csv\", indexes_reordered_by_prefix_tree)\n",
        "    else:\n",
        "      # Reorder indexes to obtain a mapping necessary to quick memory sort\n",
        "      indexing_mapping = [indexes_reordered_by_prefix_tree.index(i) for i in tqdm(range(len(indexes_reordered_by_prefix_tree)))]\n",
        "      print(indexing_mapping)\n",
        "\n",
        "      # In memory features and labels reordering\n",
        "      reorder_datastore_in_memory(features_file + \".csv\", reordered_features_file + \"_\" + str(idx) + \".csv\", indexing_mapping)\n",
        "      reorder_datastore_in_memory(labels_file + \".csv\", reordered_labels_file + \"_\" + str(idx) + \".csv\", indexing_mapping)\n",
        "\n",
        "    # Merge leaves in order to have only start and end index for a certain permutation\n",
        "    PrefixTree.merge_leaves(prefix_tree_root)\n",
        "    #PrefixTree.print_tree(prefix_tree_root)\n",
        "    prefix_trees.append(prefix_tree_root)\n",
        "\n",
        "  return prefix_trees"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aO7LU1h9K04"
      },
      "source": [
        "##Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpOySXEy9K05"
      },
      "source": [
        "def extract_features(feature_extractor, images):\n",
        "  \"\"\"\n",
        "  Extract features from a given image\n",
        "  \"\"\"\n",
        "  # Extract features\n",
        "  features = feature_extractor.predict(images, batch_size=BATCH_SIZE)\n",
        "  # Rescale between 0 and 1\n",
        "  features = minmax_scale(features,axis=1)\n",
        "  # Add a dummy element in position 0 (for compatibility with other functions)\n",
        "  features = np.column_stack([[[0]], features])\n",
        "\n",
        "  return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2mkNgpR9K05"
      },
      "source": [
        "def search_candidates(query_features, prefix_tree, pivots, Z, features_file, labels_file, precomputed_query_prefix = None):\n",
        "  \"\"\"\n",
        "  Explore the prefix tree to find candidates for a query\n",
        "  \"\"\"\n",
        "  # If the query prefix is not given, compute \n",
        "  if precomputed_query_prefix is None:\n",
        "    query_prefix = compute_permutation_prefix(query_features, PREFIX_LEN, pivots)\n",
        "  else:\n",
        "    query_prefix = precomputed_query_prefix\n",
        "  \n",
        "  # Find the smallest subtree containing at least Z elements in the leaves\n",
        "  subtree = PrefixTree.find_prefix(prefix_tree, query_prefix, Z)\n",
        "  # PrefixTree.print_tree(subtree)\n",
        "  leaves = PrefixTree.get_leaves(subtree)\n",
        "\n",
        "  # Read features and the corrispondet labels according to the subtree leaves\n",
        "  df_features = pd.read_csv(os.path.join(BASE_PP_INDEX_PATH,features_file), sep=',', header=None, skiprows= leaves[0], nrows=(leaves[-1] - leaves[0] +1)).to_numpy()\n",
        "  df_labels = pd.read_csv(os.path.join(BASE_PP_INDEX_PATH,labels_file), sep=',', header=None, skiprows= leaves[0], nrows=(leaves[-1] - leaves[0] +1)).to_numpy().reshape(-1)\n",
        "\n",
        "  return df_features, df_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH9JtU0W9K05"
      },
      "source": [
        "def k_nn_search(query_features, candidates, candidates_labels, k, from_zip = False, attachID = False):\n",
        "  '''\n",
        "  Search for k-nearest images to the query by features cosine_similarity\n",
        "  Returns paths, similarities, labels\n",
        "  '''\n",
        "  similarity_list =[]\n",
        "\n",
        "\n",
        "  # Computes cosine similarity for every candidate\n",
        "  print(f\"\\nCandidates: {len(candidates)}\")\n",
        "  for candidate in candidates:\n",
        "    dist = cosine_similarity(np.array(query_features[1:]).reshape(1,-1),np.array(candidate[1:]).reshape(1,-1))[0]\n",
        "    similarity_list.extend(dist)\n",
        "\n",
        "  # Reorder the similarity list \n",
        "  similarities_from_pivots = np.array(similarity_list)\n",
        "  sorted_list = np.argsort(similarities_from_pivots)[::-1] #reverse\n",
        "\n",
        "  paths = []\n",
        "  BASE = \"https://drive.google.com/uc?export=view&id=\"\n",
        "\n",
        "  for idx in sorted_list[:k]:\n",
        "\n",
        "    c_path = candidates[idx,0]\n",
        "\n",
        "    # If we wanto to read from drive\n",
        "    if attachID:\n",
        "      # Extract images paths\n",
        "      id=xattr.getxattr(c_path, \"user.drive.id\")\n",
        "      c_path = BASE + str(id).split(\"'\")[1]\n",
        "    \n",
        "    # If we wanto to read from zip\n",
        "    if from_zip:\n",
        "      c_path.replace('/content/gdrive/MyDrive/Data/mirflickr25k', '/content')\n",
        "      c_path.replace('/content/gdrive/MyDrive/Data/facial_expression', '/content')\n",
        "      paths.append(c_path)\n",
        "    else:\n",
        "      paths.append(c_path)\n",
        "\n",
        "  return paths, np.array(similarity_list)[sorted_list[:k]], np.array(candidates_labels)[sorted_list[:k]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2EHC-f59K06"
      },
      "source": [
        "def search(queries_features, K, pivot_file, reordered_features_file, reordered_labels_file, from_zip = False, attachID = False, query_perturbation = False, perturbation_len = 3):\n",
        "  '''\n",
        "  Search for k best images similar to queries using PP-Index\n",
        "  Returns topk_images_list, topk_scores_list, topk_labels_list\n",
        "  '''\n",
        "\n",
        "  topk_images_list = []\n",
        "  topk_scores_list = []\n",
        "  topk_labels_list = []\n",
        "\n",
        "  for query in tqdm(queries_features): \n",
        "    \n",
        "    topk_to_be_merged = np.array([[\"dummy_path\", \"-1\", \"dummy_label\"]])\n",
        "\n",
        "    # Merge all results from every PP Index if multiple are used\n",
        "    for idx, prefix_tree in enumerate(PREFIX_TREES):\n",
        "\n",
        "      # File names handling\n",
        "      pivots = np.array(pd.read_csv(os.path.join(BASE_PP_INDEX_PATH, pivot_file + \"_\" + str(idx) + \".csv\"), sep=',',header=None))\n",
        "      reordered_features = reordered_features_file + \"_\" + str(idx) + \".csv\"\n",
        "      reordered_labels = reordered_labels_file + \"_\" + str(idx) + \".csv\"\n",
        "\n",
        "      # If query perturbation is used\n",
        "      if query_perturbation:\n",
        "\n",
        "        # Compute all permutations of the first perturbation_len values of the query prefix\n",
        "        query_prefix = compute_permutation_prefix(query, PREFIX_LEN, pivots)\n",
        "        to_be_perturbated = query_prefix[:perturbation_len]\n",
        "        permutations = list(itool.permutations(to_be_perturbated))\n",
        "\n",
        "        # Search candidates for the first permutation. Save the candidates together with their labels\n",
        "        query_prefix[:perturbation_len] = np.array(permutations[0])\n",
        "        new_candidates, candidates_labels = search_candidates(query, prefix_tree, pivots, N_CANDIDATES, reordered_features, reordered_labels, query_prefix)\n",
        "        candidates = np.column_stack([np.array(candidates_labels).reshape(-1,1), np.array(new_candidates)])\n",
        "\n",
        "        # Search candidates for all the other permutations. Save the candidates together with their labels\n",
        "        for permutation in permutations[1:]:\n",
        "\n",
        "          query_prefix[:perturbation_len] = np.array(permutation)\n",
        "          new_candidates, candidates_labels = search_candidates(query, prefix_tree, pivots, N_CANDIDATES, reordered_features, reordered_labels, query_prefix)\n",
        "          # Merge candidates for all the permutations\n",
        "          candidates = np.concatenate((np.column_stack([np.array(candidates_labels).reshape(-1,1), np.array(new_candidates)]), candidates))\n",
        "\n",
        "        # Remove dupliacates and separate candidates labels from candidates\n",
        "        unique_candidates = np.unique(candidates.astype(\"str\"), axis=0)\n",
        "        candidates_labels = unique_candidates[:,0].astype('int') \n",
        "        candidates = unique_candidates[:,1:]\n",
        "\n",
        "      # Query perturbation is not used\n",
        "      else:\n",
        "        candidates, candidates_labels = search_candidates(query, prefix_tree, pivots, N_CANDIDATES, reordered_features, reordered_labels)\n",
        "\n",
        "      # k_nn_search to obtain topk_paths, topk_scores, topk_lables\n",
        "      topk_paths,  topk_scores, topk_labels= k_nn_search(query, candidates, candidates_labels, K, from_zip, attachID)\n",
        "\n",
        "      # Concatenate top k results obtained from each index\n",
        "      topk_to_be_merged = np.concatenate((np.array(list(zip(topk_paths,topk_scores,topk_labels))), topk_to_be_merged))\n",
        "    \n",
        "    # Remove duplicates and sort by score\n",
        "    unique_topk = np.unique(topk_to_be_merged.astype(\"str\"), axis=0)\n",
        "    unique_topk = unique_topk[unique_topk[:,1].argsort()][::-1]\n",
        "\n",
        "    # Take top k results\n",
        "    topk_paths = unique_topk[:K,0]\n",
        "    topk_scores = unique_topk[:K,1].astype('float64') \n",
        "    topk_labels = unique_topk[:K,2].astype('int')\n",
        "\n",
        "    # If we read from drive\n",
        "    if not attachID:\n",
        "      topk_images = [img_to_array(load_img(path, target_size=(PREVIEW_SIZE,PREVIEW_SIZE))) for path in  topk_paths]\n",
        "      topk_images_list.append(topk_images)\n",
        "    else:\n",
        "      topk_images_list.append(topk_paths)    \n",
        "\n",
        "    topk_scores_list.append(topk_scores)\n",
        "    topk_labels_list.append(topk_labels)\n",
        "\n",
        "  return topk_images_list, topk_scores_list, topk_labels_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm0lHusE9K06"
      },
      "source": [
        "##Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejO83HIQ9K06"
      },
      "source": [
        "# utility to resize, pad, and write score on images\n",
        "def process_images(image, score, is_relevant):\n",
        "  image = img_to_array(image)\n",
        "\n",
        "  # to uint8\n",
        "  # image = ((image + 1) * 127.5).astype(np.uint8)\n",
        "  image = image.astype(np.uint8)\n",
        "\n",
        "  # add a red/green flag\n",
        "  color = (0, 255, 0) if is_relevant else (255, 0, 0)\n",
        "  flag = np.full((10, PREVIEW_SIZE, 3), fill_value=color, dtype=image.dtype)\n",
        "  image = np.concatenate((image, flag), axis=0)\n",
        "\n",
        "  # resize\n",
        "  image = Image.fromarray(image).convert('RGBA')\n",
        "  image.thumbnail((PREVIEW_SIZE, PREVIEW_SIZE))  # use PIL to resize the image\n",
        "\n",
        "  # draw score\n",
        "  draw = ImageDraw.Draw(image)\n",
        "  draw.text((3, PREVIEW_SIZE - 12), f'{score:.2f}', anchor='lt', fill=(255, 255, 255, 255))\n",
        "\n",
        "  # pad the image with transparency\n",
        "  image = ImageOps.expand(image, 5, fill=(0, 0, 0, 0))\n",
        "  image = np.array(image)\n",
        "  return image\n",
        "\n",
        "# use np.vectorize to apply custom functions to numpy arrays\n",
        "np_process_image = np.vectorize(process_images, signature='(h,w,c),(),()->(h1,w1,c1)')\n",
        "\n",
        "# utility function to draw knn results for multiple queries\n",
        "def show_results(images, scores, is_relevant):\n",
        "  # images has shape (n_queries, k, H, W, C)\n",
        "  images = np_process_image(images, scores, is_relevant)\n",
        "  images = np.concatenate(images, axis=1)  # concatenate queries vertically\n",
        "  images = np.concatenate(images, axis=1)  # concatenate results horizontally\n",
        "  display(Image.fromarray(images))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW4GrsDY9K06"
      },
      "source": [
        "def evaluate_knn_search(queries_features, queries_labels, k, pivot_file, reordered_features_file, reordered_labels_file, from_zip=False, query_perturbation=False):\n",
        "  '''\n",
        "  Evaluate APs and mean AP of all queries\n",
        "  '''\n",
        "\n",
        "  # Search \n",
        "  topk_images, topk_scores, topk_labels = search(queries_features, k, pivot_file, reordered_features_file, reordered_labels_file, from_zip, query_perturbation = query_perturbation)\n",
        "  # If a certain retreived image is relevant \n",
        "  topk_is_relevant = np.array(topk_labels) == queries_labels.reshape(-1,1)\n",
        "\n",
        "  # Average Precision Score computation\n",
        "  aps = np.array([sklearn.metrics.average_precision_score(l, s) for l,s in zip(topk_is_relevant, topk_scores)])\n",
        "  aps[np.isnan(aps)] = 0\n",
        "  print('APs per Query:', aps)\n",
        "  print('mAP:', np.mean(aps))\n",
        "\n",
        "  # Shows a result grid\n",
        "  show_results(topk_images, topk_scores, topk_is_relevant)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8KycNp2KaZH"
      },
      "source": [
        "#STEP 2: FLASK WEB USER INTERFACE\n",
        "In order to create the Web App, *Flask* is employed. *Ngrok* is used to expose the server running on the virtual machine to a public URL, as described in [ngrok tutorial](https://medium.com/@kshitijvijay271199/flask-on-google-colab-f6525986797b#:~:text=Here%20comes%20the%20Python%20library%20flask%2Dngrok.&text=A%20secure%20URL%20is%20provided,before%20deploying%20it%20into%20production.)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrwnDK0nKaZH",
        "outputId": "c523c93c-ef95-4f46-bb70-f64046b70092"
      },
      "source": [
        "\"\"\"\n",
        "MODEL LOADING AND VARIABLES DEFINITION\n",
        "Load the model that will be used to extract the features from the query image.\n",
        "\"\"\"\n",
        "\n",
        "model = load_model(os.path.join(MODELS_PATH, \"facenet_ft2.h5\"))\n",
        "\n",
        "K = 100\n",
        "N_CANDIDATES = 300\n",
        "BASE_PP_INDEX_PATH = \"/content/gdrive/My Drive/Data/PP-Index\"\n",
        "FEATURES_PATH = \"/content/gdrive/My Drive/Data/features\"\n",
        "PREFIX_LEN = 4\n",
        "CHUNKSIZE = 1000\n",
        "\n",
        "prefix_tree_web = PrefixTree(\"Prefix Tree WEB\")\n",
        "PrefixTree.add_from_csv(prefix_tree_web, \"finetuned_permutations_0.csv\", PREFIX_LEN, chunksize = CHUNKSIZE)\n",
        "PrefixTree.merge_leaves(prefix_tree_web)\n",
        "PREFIX_TREES = [prefix_tree_web]\n",
        "REORDERED_FEATURES = \"finetuned_reordered_features\"\n",
        "REORDERED_LABELS = \"finetuned_reordered_labels\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "58it [00:01, 49.22it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jlXyuhxAkpn"
      },
      "source": [
        "\"\"\"\n",
        "PREPROCESSING FUNCTION DEFINITION\n",
        "This function will be used to perform the preprocessing on the query image. \n",
        "The type of preprocessing used is exactly the same used in feature extraction phase.\n",
        "\"\"\"\n",
        "\n",
        "def preprocess(images, labels):\n",
        "  # rescales from [0, 255] to [-1, 1], equivalent to:  images = (images / 127.5) - 1\n",
        "  images = tf.keras.applications.mobilenet_v2.preprocess_input(images)\n",
        "  return images, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "5S5zr1f4DnWq",
        "outputId": "8d5b711d-7a45-4b82-ebbc-93ba1959493f"
      },
      "source": [
        "from google.colab.output import eval_js\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(5000)\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://ttexvyhmeij-496ff2e9c6d22116-5000-colab.googleusercontent.com/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xsg8iFBLKaZI",
        "outputId": "db0e3624-0afa-4b54-b754-ac318536604d"
      },
      "source": [
        "\"\"\"\n",
        "WEB APP DEFINITION AND DEPLOYING\n",
        "In order to use the Web App, click on the second link.\n",
        "\"\"\"\n",
        "\n",
        "app = Flask(__name__,template_folder='/content/gdrive/MyDrive/templates',static_url_path='/content/gdrive/MyDrive/static')\n",
        "run_with_ngrok(app)\n",
        "app.config['UPLOAD_EXTENSIONS'] = ['.jpg', '.png', '.gif']\n",
        "results=[]\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    print(app.instance_path)\n",
        "    return render_template('index.html',results=results)\n",
        "\n",
        "@app.route('/search',methods=['POST'])\n",
        "def web_search():\n",
        "    uploaded_file = request.files['query']\n",
        "    filename = uploaded_file.filename\n",
        "\n",
        "    #remove all files from upload directory\n",
        "    files = glob.glob('/content/gdrive/MyDrive/static/upload/*')\n",
        "    for f in files:\n",
        "      os.remove(f)\n",
        "    \n",
        "    if filename != '':\n",
        "\n",
        "        #the uploaded file is accepted only if its extension is .jpg, .png or .gif\n",
        "        file_ext = os.path.splitext(filename)[1]\n",
        "        if file_ext not in app.config['UPLOAD_EXTENSIONS']:\n",
        "            abort(400)\n",
        "        \n",
        "        #save query file in upload directory, necessary to correctly reconstruct it into a tensor\n",
        "        file_path = os.path.join('/content/gdrive/MyDrive/static/upload', filename)\n",
        "        uploaded_file.save(file_path)\n",
        "\n",
        "        #read query image using image_dataset_from_directory, in order to perform the same preprocessing applied for feature extraction\n",
        "        queryds, query_path = image_dataset_from_directory(directory='/content/gdrive/MyDrive/static/', labels=\"inferred\", label_mode=\"int\", color_mode=\"rgb\", batch_size=BATCH_SIZE, image_size=(160,160), shuffle=False, seed = SEED)\n",
        "        queryds = queryds.map(preprocess, deterministic=True)\n",
        "\n",
        "        #extract features from query image using the specified model\n",
        "        query_features = extract_features(model, queryds)\n",
        "\n",
        "        #search for results \n",
        "        paths, scores, labels = search(query_features, K, pivot_file=\"finetuned_pivots\", reordered_features_file=REORDERED_FEATURES, reordered_labels_file=REORDERED_LABELS, attachID = True)\n",
        "\n",
        "        #transform the format so that it is understandable from html file\n",
        "        results = [ [x, np.round(y,3), CATEGORICAL_LABELS[z]] for x,y,z in zip(paths[0], scores[0], labels[0])]\n",
        "  \n",
        "    return render_template('index.html',results=results)\n",
        "\n",
        "if __name__== \"__main__\":\n",
        "  app.run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
            "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://7837afb7c7af.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [29/Jan/2021 07:41:14] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/instance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [29/Jan/2021 07:41:15] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 1 files belonging to 1 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Candidates: 1736\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:47<00:00, 47.21s/it]\n",
            "127.0.0.1 - - [29/Jan/2021 07:42:29] \"\u001b[37mPOST /search HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [29/Jan/2021 07:42:58] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 1 files belonging to 1 classes.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Candidates: 1148\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}