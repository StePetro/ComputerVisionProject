{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "1jPpAogICK1I"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StePetro/ComputerVisionProject/blob/main/Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dB5aDAMPEW7p"
      },
      "source": [
        "#SETUP ENVIRONMENT "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi_kPYiKshFq"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJsdJDtXd99y"
      },
      "source": [
        "!apt-get install -qq xattr\n",
        "!pip install scikit-learn-extra\n",
        "!pip install flask-ngrok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSiRB65Du0GV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f13a52bc-ee04-44fa-c8ec-09b393535551"
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "import numpy as np\n",
        "import glob\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.callbacks import *\n",
        "import tensorflow.keras.backend as KK\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from IPython.display import display\n",
        "from keras.regularizers import l2\n",
        "from matplotlib.pyplot import imshow\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from PIL import Image, ImageDraw, ImageOps\n",
        "from IPython.display import display\n",
        "from time import sleep\n",
        "import sklearn\n",
        "import copy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.preprocessing import normalize, minmax_scale\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask,render_template,request, abort\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.python.data.ops import dataset_ops\n",
        "from tensorflow.python.keras.layers.preprocessing import image_preprocessing\n",
        "from tensorflow.python.keras.preprocessing import dataset_utils\n",
        "from tensorflow.python.ops import image_ops\n",
        "from tensorflow.python.ops import io_ops\n",
        "from tensorflow.python.util.tf_export import keras_export\n",
        "import xattr\n",
        "\n",
        "import os\n",
        "import datetime\n",
        "from numpy import genfromtxt\n",
        "from csv import reader\n",
        "import pandas as pd\n",
        "import bisect\n",
        "from numpy import save\n",
        "import scipy as sc\n",
        "from tqdm import tqdm\n",
        "import heapq\n",
        "import random\n",
        "import itertools as itool\n",
        "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
        "\n",
        "DATA_PATH = '/content/gdrive/My Drive/Data/'\n",
        "MODELS_PATH = DATA_PATH + 'models'\n",
        "FACIAL_EXPRESSION_PATH = '/content/dataset'\n",
        "MIRFLICKR_PATH = \"/content/distractor\"\n",
        "FEATURES_PATH = DATA_PATH + \"features\"\n",
        "CATEGORICAL_LABELS = {0:\"Angry\", 1:\"Disgust\", 2:\"Fear\", 3:\"Happy\", 4:\"Neutral\", 5:\"Sad\", 6:\"Surprise\", 99:\"Distractor\"}\n",
        "FACIAL_EXPRESSION_SETS = [\"Training\", \"PrivateTest\", \"PublicTest\"]\n",
        "WHITELIST_FORMATS = ('.bmp', '.gif', '.jpeg', '.jpg', '.png')\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = BATCH_SIZE\n",
        "VALIDATION_SPLIT = 0.2\n",
        "SEED = 76\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khZglff7HHif"
      },
      "source": [
        "!unzip '/content/gdrive/My Drive/Data/facial_expression.zip' -d \"/content\"\n",
        "!unzip '/content/gdrive/My Drive/Data/distractor.zip' -d \"/content\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtpuYu6AyU2b"
      },
      "source": [
        "#DATASET LOADING AND PREPROCESSING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leF0CyzbMVk8"
      },
      "source": [
        "In order to mantain filenames corresponding to the images in the dataset, it is necessary to override the *image_dataset_from_directory* function according to the code at the following link: [image_dataset_from_directory_override](https://github.com/tensorflow/tensorflow/blob/v2.3.0/tensorflow/python/keras/preprocessing/image_dataset.py#L34-L206)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StVChiwyMMUQ"
      },
      "source": [
        "\"\"\"\n",
        "IMAGE_DATASET_FROM_DIRECTORY FUNCTION OVERRIDE\n",
        "\"\"\"\n",
        "\n",
        "def image_dataset_from_directory(directory,\n",
        "                                 labels='inferred',\n",
        "                                 label_mode='int',\n",
        "                                 class_names=None,\n",
        "                                 color_mode='rgb',\n",
        "                                 batch_size=32,\n",
        "                                 image_size=(256, 256),\n",
        "                                 shuffle=True,\n",
        "                                 seed=None,\n",
        "                                 validation_split=None,\n",
        "                                 subset=None,\n",
        "                                 interpolation='bilinear',\n",
        "                                 follow_links=False):\n",
        "  \n",
        "  if labels != 'inferred':\n",
        "    if not isinstance(labels, (list, tuple)):\n",
        "      raise ValueError(\n",
        "          '`labels` argument should be a list/tuple of integer labels, of '\n",
        "          'the same size as the number of image files in the target '\n",
        "          'directory. If you wish to infer the labels from the subdirectory '\n",
        "          'names in the target directory, pass `labels=\"inferred\"`. '\n",
        "          'If you wish to get a dataset that only contains images '\n",
        "          '(no labels), pass `label_mode=None`.')\n",
        "    if class_names:\n",
        "      raise ValueError('You can only pass `class_names` if the labels are '\n",
        "                       'inferred from the subdirectory names in the target '\n",
        "                       'directory (`labels=\"inferred\"`).')\n",
        "  if label_mode not in {'int', 'categorical', 'binary', None}:\n",
        "    raise ValueError(\n",
        "        '`label_mode` argument must be one of \"int\", \"categorical\", \"binary\", '\n",
        "        'or None. Received: %s' % (label_mode,))\n",
        "  if color_mode == 'rgb':\n",
        "    num_channels = 3\n",
        "  elif color_mode == 'rgba':\n",
        "    num_channels = 4\n",
        "  elif color_mode == 'grayscale':\n",
        "    num_channels = 1\n",
        "  else:\n",
        "    raise ValueError(\n",
        "        '`color_mode` must be one of {\"rbg\", \"rgba\", \"grayscale\"}. '\n",
        "        'Received: %s' % (color_mode,))\n",
        "  interpolation = image_preprocessing.get_interpolation(interpolation)\n",
        "  dataset_utils.check_validation_split_arg(\n",
        "      validation_split, subset, shuffle, seed)\n",
        "\n",
        "  if seed is None:\n",
        "    seed = np.random.randint(1e6)\n",
        "  image_paths, labels, class_names = dataset_utils.index_directory(\n",
        "      directory,\n",
        "      labels,\n",
        "      formats=WHITELIST_FORMATS,\n",
        "      class_names=class_names,\n",
        "      shuffle=shuffle,\n",
        "      seed=seed,\n",
        "      follow_links=follow_links)\n",
        "\n",
        "  if label_mode == 'binary' and len(class_names) != 2:\n",
        "    raise ValueError(\n",
        "        'When passing `label_mode=\"binary\", there must exactly 2 classes. '\n",
        "        'Found the following classes: %s' % (class_names,))\n",
        "\n",
        "  image_paths, labels = dataset_utils.get_training_or_validation_split(\n",
        "      image_paths, labels, validation_split, subset)\n",
        "\n",
        "  dataset = paths_and_labels_to_dataset(\n",
        "      image_paths=image_paths,\n",
        "      image_size=image_size,\n",
        "      num_channels=num_channels,\n",
        "      labels=labels,\n",
        "      label_mode=label_mode,\n",
        "      num_classes=len(class_names),\n",
        "      interpolation=interpolation)\n",
        "  if shuffle:\n",
        "    # Shuffle locally at each iteration\n",
        "    dataset = dataset.shuffle(buffer_size=batch_size * 8, seed=seed)\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  # Users may need to reference `class_names`.\n",
        "  dataset.class_names = class_names\n",
        "  return dataset, image_paths\n",
        "\n",
        "def paths_and_labels_to_dataset(image_paths,\n",
        "                                image_size,\n",
        "                                num_channels,\n",
        "                                labels,\n",
        "                                label_mode,\n",
        "                                num_classes,\n",
        "                                interpolation):\n",
        "  \"\"\"Constructs a dataset of images and labels.\"\"\"\n",
        "  path_ds = dataset_ops.Dataset.from_tensor_slices(image_paths)\n",
        "  img_ds = path_ds.map(\n",
        "      lambda x: path_to_image(x, image_size, num_channels, interpolation))\n",
        "  if label_mode:\n",
        "    label_ds = dataset_utils.labels_to_dataset(labels, label_mode, num_classes)\n",
        "    img_ds = dataset_ops.Dataset.zip((img_ds, label_ds))\n",
        "  return img_ds\n",
        "\n",
        "\n",
        "def path_to_image(path, image_size, num_channels, interpolation):\n",
        "  img = io_ops.read_file(path)\n",
        "  img = image_ops.decode_image(\n",
        "      img, channels=num_channels, expand_animations=False)\n",
        "  img = image_ops.resize_images_v2(img, image_size, method=interpolation)\n",
        "  img.set_shape((image_size[0], image_size[1], num_channels))\n",
        "  return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1ut60eNI5iR"
      },
      "source": [
        "\"\"\"\n",
        "LOAD FACIAL EXPRESSION DATASET\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "training_dataset_noshuffle, training_dataset_noshuffle_path = image_dataset_from_directory(directory=FACIAL_EXPRESSION_PATH+\"/Training\", labels=\"inferred\", label_mode=\"int\", \n",
        "                                                          color_mode=\"rgb\", batch_size=BATCH_SIZE, image_size=(160,160), shuffle=False, seed = SEED)\n",
        "  \n",
        "public_test_dataset, public_test_dataset_path = image_dataset_from_directory(directory=FACIAL_EXPRESSION_PATH+\"/PublicTest\", labels=\"inferred\", label_mode=\"int\", color_mode=\"rgb\",\n",
        "                                                  batch_size=BATCH_SIZE, image_size=(160,160), shuffle=False, seed = SEED)\n",
        "  \n",
        "private_test_dataset, private_test_dataset_path = image_dataset_from_directory(directory=FACIAL_EXPRESSION_PATH+\"/PrivateTest\", labels=\"inferred\", label_mode=\"int\", color_mode=\"rgb\",\n",
        "                                                  batch_size=BATCH_SIZE, image_size=(160,160), shuffle=False, seed = SEED)\n",
        "\n",
        "train_datagen =  tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,   \n",
        "    validation_split=0.2) \n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    FACIAL_EXPRESSION_PATH+\"/Training\",\n",
        "    target_size=(160,160),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    color_mode=\"rgb\",\n",
        "    class_mode='sparse',\n",
        "    shuffle=True,\n",
        "    seed=SEED,\n",
        "    subset='training') # set as training data\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    FACIAL_EXPRESSION_PATH+\"/Training\", \n",
        "    target_size=(160,160),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    color_mode=\"rgb\",\n",
        "    class_mode='sparse',\n",
        "    seed=SEED,\n",
        "    subset='validation')\n",
        "\n",
        "training_dataset_noshuffle_path =[s.replace('/content','/content/gdrive/MyDrive/Data/facial_expression') for s in training_dataset_noshuffle_path ]\n",
        "public_test_dataset_path =[s.replace('/content','/content/gdrive/MyDrive/Data/facial_expression') for s in public_test_dataset_path ]\n",
        "private_test_dataset_path =[s.replace('/content','/content/gdrive/MyDrive/Data/facial_expression') for s in private_test_dataset_path ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAmTTIa8yg8n",
        "outputId": "4993c9e0-72de-4a25-a5ce-aedd8d7210c1"
      },
      "source": [
        "\"\"\"\n",
        "LOAD MIRFLICKR25 DATASET\n",
        "\"\"\"\n",
        "\n",
        "mirflickr_dataset, mirflickr_dataset_path = image_dataset_from_directory(directory=MIRFLICKR_PATH, labels=\"inferred\", label_mode=\"int\", color_mode=\"rgb\", batch_size=BATCH_SIZE, image_size=(160,160), shuffle=False, seed=SEED)\n",
        "mirflickr_dataset_path =[s.replace('/content','/content/gdrive/MyDrive/Data/mirflickr25k') for s in mirflickr_dataset_path ]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdTUbMlg9Kep"
      },
      "source": [
        "\"\"\"\n",
        "DATASET NORMALIZATION\n",
        "\"\"\"\n",
        "\n",
        "def preprocess(images, labels):\n",
        "  # rescales from [0, 255] to [-1, 1], equivalent to:  images = (images / 127.5) - 1\n",
        "  images = tf.keras.applications.mobilenet_v2.preprocess_input(images)\n",
        "  return images, labels\n",
        "\n",
        "# apply prepocessing to datasets (lazy operation)\n",
        "\n",
        "training_dataset_noshuffle = training_dataset_noshuffle.map(preprocess, deterministic=True)\n",
        "public_test_dataset = public_test_dataset.map(preprocess, deterministic=True)\n",
        "private_test_dataset = private_test_dataset.map(preprocess, deterministic=True)\n",
        "mirflickr_dataset = mirflickr_dataset.map(preprocess, deterministic=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYPsMEhy96Lp"
      },
      "source": [
        "\"\"\"\n",
        "PREFETCH\n",
        "Prefetch is used in order to speed up operations such as feature extraction.\n",
        "\"\"\"\n",
        "\n",
        "training_dataset_noshuffle = training_dataset_noshuffle.prefetch(buffer_size=BUFFER_SIZE)\n",
        "public_test_dataset = public_test_dataset.prefetch(buffer_size=BUFFER_SIZE)\n",
        "private_test_dataset = private_test_dataset.prefetch(buffer_size=BUFFER_SIZE)\n",
        "mirflickr_dataset = mirflickr_dataset.prefetch(buffer_size=BUFFER_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPkdELO_DevK"
      },
      "source": [
        "\"\"\"\n",
        "WRITE LABELS IN labels.csv FILE\n",
        "\"\"\"\n",
        "\n",
        "distractorlabels=np.full((25000,1),99)\n",
        "\n",
        "with open(os.path.join(FEATURES_PATH, \"labels.csv\"),\"ab\") as f:\n",
        "\n",
        "  np.savetxt(f, np.concatenate([y for x, y in training_dataset_noshuffle], axis=0), delimiter=\",\", fmt=\"%s\")\n",
        "  np.savetxt(f, np.concatenate([y for x, y in public_test_dataset], axis=0), delimiter=\",\", fmt=\"%s\")\n",
        "  #np.savetxt(f, np.concatenate([y for x, y in private_test_dataset], axis=0), delimiter=\",\", fmt=\"%s\")\n",
        "  np.savetxt(f, distractorlabels, delimiter=\",\", fmt=\"%s\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O28KA7s1ydGx"
      },
      "source": [
        "#STEP 1: PP-INDEX IMPLEMENTATION\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEapMP4DD7e7"
      },
      "source": [
        "##Permutations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_Nr7oqwEgYV"
      },
      "source": [
        "###Pivots Choice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHyPm5UHD7HV"
      },
      "source": [
        "def extract_random_pivots(input_file, pivots_file, N):\n",
        "  \"\"\"\n",
        "  Extract N random pivots from input_file and save them in output_file\n",
        "  \"\"\"\n",
        "  \n",
        "  #Extract N rows at random without opening the entire file\n",
        "  with open(os.path.join(FEATURES_PATH,input_file)) as fin:\n",
        "      sample = heapq.nlargest(N, fin, key=lambda L: random.random())\n",
        "\n",
        "  # Delete old pivots_file\n",
        "  if os.path.isfile(os.path.join(BASE_PP_INDEX_PATH,pivots_file)):\n",
        "   os.remove(os.path.join(BASE_PP_INDEX_PATH,pivots_file))\n",
        "\n",
        "  #Save new pivots_file\n",
        "  with open(os.path.join(BASE_PP_INDEX_PATH,pivots_file), 'w+') as piv:\n",
        "    for s in sample:\n",
        "       piv.write(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNuod3SL200A"
      },
      "source": [
        "def extract_k_medoids_pivots(input_file, pivots_file, aproximation_size, N):\n",
        "  \"\"\"\n",
        "  Extract N pivots from input_file unsing in memory k-medoids algorithm and save them in output_file\n",
        "  \"\"\"\n",
        "  \n",
        "  #Extract N rows at random without opening the entire file\n",
        "  with open(os.path.join(FEATURES_PATH,input_file)) as fin:\n",
        "      strings_sample = heapq.nlargest(aproximation_size, fin, key=lambda L: random.random())\n",
        "\n",
        "  # Parse to remove the string path from the samples\n",
        "  sample = np.array([np.float64(s.split(\",\")[1:]) for s in strings_sample])\n",
        "\n",
        "  # Take the indexes of the cluster centers\n",
        "  pivots_index = KMedoids(N,\"cosine\").fit(sample[:,1:]).medoid_indices_\n",
        "\n",
        "  # Use the clusters centers as pivots\n",
        "  pivots = sample[pivots_index]\n",
        "  pivots = np.column_stack([np.zeros(N).reshape(-1,1), pivots])\n",
        "\n",
        "  # Delete old pivots_file\n",
        "  if os.path.isfile(os.path.join(BASE_PP_INDEX_PATH,pivots_file)):\n",
        "    os.remove(os.path.join(BASE_PP_INDEX_PATH,pivots_file))\n",
        "\n",
        "  #Save new pivots_file\n",
        "  with open(os.path.join(BASE_PP_INDEX_PATH,pivots_file), 'w+') as piv:\n",
        "    np.savetxt(piv,pivots,delimiter=\",\")     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5X10knkLEku9"
      },
      "source": [
        "###Permutation Computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGFTmIBnEuEK"
      },
      "source": [
        "def create_permutation_dataset(input_file, output_file, pivots_file, chunksize):\n",
        "  \"\"\"\n",
        "  Compute pivots permutation for each element and save it in a file\n",
        "  \"\"\"\n",
        "\n",
        "  #Take pivots\n",
        "  df_pivots=pd.read_csv(os.path.join(BASE_PP_INDEX_PATH,pivots_file), sep=',',header=None )\n",
        "  pivots = df_pivots.to_numpy()\n",
        "\n",
        "  #Take features\n",
        "  df_features=pd.read_csv(os.path.join(FEATURES_PATH,input_file), sep=',',header=None, chunksize = chunksize)\n",
        "\n",
        "  # Delete old permutation file\n",
        "  if os.path.isfile(os.path.join(BASE_PP_INDEX_PATH,output_file)):\n",
        "    os.remove(os.path.join(BASE_PP_INDEX_PATH,output_file))\n",
        "\n",
        "  #Compute permutations\n",
        "  for chunk in tqdm(df_features):\n",
        "    chunk = np.array(chunk)\n",
        "\n",
        "    distances_list_chunk = []\n",
        "    for elem in chunk: \n",
        "\n",
        "      distances_list = []\n",
        "\n",
        "      #Compute distance list between elem and pivots\n",
        "      for piv in pivots:\n",
        "\n",
        "        dist = cosine_distances(np.array(elem[1:]).reshape(1,-1),np.array(piv[1:]).reshape(1,-1))[0]\n",
        "        distances_list.extend(dist)\n",
        "        distances_from_pivots = np.array(distances_list)\n",
        "\n",
        "      # Reorder the distance list (to obtain the permutation)\n",
        "      distances_list_chunk.append(np.array(pd.DataFrame(np.argsort(distances_from_pivots)).T).reshape(-1))\n",
        "\n",
        "    # Save permutations to csv file \n",
        "    pd.DataFrame(distances_list_chunk).to_csv(os.path.join(BASE_PP_INDEX_PATH,output_file), mode='a', index = False, header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q76Lp9dDGOd7"
      },
      "source": [
        "def compute_permutation_prefix(query, dim_prefix, pivots):\n",
        "  \"\"\"\n",
        "  Compute prefix of a given query \n",
        "  \"\"\"\n",
        "\n",
        "  permutations_list=[]\n",
        "  distances_list = []\n",
        "\n",
        "  # Compute distance list between query and pivots \n",
        "  for piv in pivots:\n",
        "    dist = cosine_distances(np.array(query[1:]).reshape(1,-1),np.array(piv[1:]).reshape(1,-1))[0]\n",
        "    distances_list.extend(dist)\n",
        "  \n",
        "  # Compute permutation: reoder the distance list \n",
        "  distances_from_pivots = np.array(distances_list)\n",
        "  permutations_list.extend(np.argsort(distances_from_pivots))\n",
        "  # Return the prefix (=the first dim_prefix elements of permutation)\n",
        "  return np.array(permutations_list)[:dim_prefix]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jPpAogICK1I"
      },
      "source": [
        "##Prefix Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VfDMRMTGzXP"
      },
      "source": [
        "class PrefixTree(object):\n",
        "    \"\"\"\n",
        "    Prefix tree implementation\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, value: str, is_leaf = False, is_last_pivot = False):\n",
        "        # Value of the node\n",
        "        self.value = value\n",
        "        self.children = []\n",
        "        # If this is a leaf node\n",
        "        self.is_leaf = is_leaf\n",
        "        self.is_last_pivot = is_last_pivot\n",
        "        # How many images have this prefix\n",
        "        self.counter = 1\n",
        "    \n",
        "\n",
        "    def add(root, permutation, position_index):\n",
        "        \"\"\"\n",
        "        Adding a permutation in the prefix tree structure\n",
        "        \"\"\"\n",
        "        node = root\n",
        "        for idx, pivot in enumerate(permutation):\n",
        "            found_in_child = False\n",
        "            # Search for the pivot in the children of the present `node`\n",
        "            for child in node.children:\n",
        "                if child.value == pivot:\n",
        "                    # Increase the counter by 1 to keep track that another\n",
        "                    # permutation has hit\n",
        "                    child.counter += 1\n",
        "                    # Point node to the child that contains this pivot\n",
        "                    node = child\n",
        "                    found_in_child = True                    \n",
        "            # If pivot is not find add a new chlid\n",
        "            if not found_in_child:\n",
        "                new_node = PrefixTree(pivot)\n",
        "                node.children.append(new_node)\n",
        "                # Point node to the new child\n",
        "                node = new_node\n",
        "            if idx == (len(permutation) - 1):\n",
        "                #last\n",
        "                node.children.append(PrefixTree(position_index, is_leaf = True))\n",
        "            # Last prefix's pivot\n",
        "        node.is_last_pivot = True\n",
        "\n",
        "\n",
        "    def find_prefix(root, prefix: str, k):\n",
        "      \"\"\"\n",
        "      Find the prefix subtree with that prefix and at least k leaves\n",
        "      \"\"\"\n",
        "      node = root\n",
        "      # If the root node has no children, then return False.\n",
        "      if not root.children:\n",
        "        return False\n",
        "\n",
        "      for pivot in prefix:\n",
        "        pivot_not_found = True\n",
        "\n",
        "        # Search through all the children\n",
        "        for child in node.children:\n",
        "                  \n",
        "          if child.value == pivot:\n",
        "            pivot_not_found = False        \n",
        "            if(child.counter < k):\n",
        "              # If child leaves < k return parent to have at least k leaves\n",
        "              return node\n",
        "            node = child\n",
        "            break\n",
        "\n",
        "        # Return False anyway when not find a pivot\n",
        "        if pivot_not_found:\n",
        "          return False\n",
        "\n",
        "      return node\n",
        "\n",
        "    def get_leaves(node):\n",
        "      \"\"\"\n",
        "      Get prefix tree's leaves in order\n",
        "      \"\"\"\n",
        "      leaves = []\n",
        "      # Check each children of a node\n",
        "      for child in node.children:\n",
        "        if child.is_leaf:\n",
        "          # Take the leaf value\n",
        "          leaves.append(child.value)\n",
        "        else:\n",
        "          leaves.extend(PrefixTree.get_leaves(child))\n",
        "      return leaves\n",
        "\n",
        "    \n",
        "    def extract_parent_pointers(node):\n",
        "      \"\"\"\n",
        "      Extract leaves parents\n",
        "      \"\"\"\n",
        "      parent_pointers = []\n",
        "      # Check each children of a node\n",
        "      for child in node.children:\n",
        "        if child.is_last_pivot:  \n",
        "          # If is_last_pivot = True it means that the node children are leaves\n",
        "          parent_pointers.append(child) \n",
        "        if not child.is_last_pivot:\n",
        "          parent_pointers.extend(PrefixTree.extract_parent_pointers(child))\n",
        "      return parent_pointers\n",
        "\n",
        "\n",
        "    def merge_leaves(node):\n",
        "      \"\"\"\n",
        "      Merge pointers to reflect new order\n",
        "      \"\"\"\n",
        "      parent_pointers = PrefixTree.extract_parent_pointers(node)\n",
        "      #Use idx to save how many elements merged in the leaves \n",
        "      idx = 0\n",
        "      # For each leaf parent\n",
        "      for parent in parent_pointers:\n",
        "        # If there are more than one leaf\n",
        "        if len(parent.children)!= 1:\n",
        "          # Merge and save start-idx and end-idx\n",
        "          size = len(parent.children)\n",
        "          parent.children = [ PrefixTree(idx, is_leaf = True),  \n",
        "                              PrefixTree(idx + len(parent.children) - 1, is_leaf = True) ]\n",
        "          idx += size\n",
        "        else:\n",
        "          # Only one element in leaf\n",
        "          parent.children = [ PrefixTree(idx, is_leaf = True)]  \n",
        "          idx += 1\n",
        "\n",
        " \n",
        "    def print_tree(node, _prefix=\"\", _last=True):\n",
        "      \"\"\"\n",
        "      Print prefix tree's structure\n",
        "      \"\"\"\n",
        "      print(_prefix, \"`- \" if _last else \"|- \", node.value, sep=\"\")\n",
        "      _prefix += \"   \" if _last else \"|  \"\n",
        "      child_count = len(node.children)\n",
        "      for i, child in enumerate(node.children):\n",
        "        _last = i == (child_count - 1)\n",
        "        PrefixTree.print_tree(child, _prefix, _last)\n",
        "\n",
        "  \n",
        "    def add_all(root, permutations, prefix_len, base_index = 0):\n",
        "      '''\n",
        "      Insert in the tree all the permutations cutting them according to prefix_len\n",
        "      '''\n",
        "      for index, prefix in enumerate(permutations):\n",
        "        PrefixTree.add(root, np.array(prefix).reshape(-1)[:prefix_len], base_index + index)\n",
        "\n",
        "\n",
        "    def add_from_csv(root, permutation_file, prefix_len, chunksize):\n",
        "      \"\"\"\n",
        "      Insert in the tree all the permutations cutting them according to prefix_len\n",
        "      from csv permutation_file in chunks of chunksize\n",
        "      \"\"\"\n",
        "      permutations_chunks = pd.read_csv(os.path.join(BASE_PP_INDEX_PATH,permutation_file), sep=',', header=None, chunksize = chunksize)\n",
        "      for i, permutations_chunk in tqdm(enumerate(permutations_chunks)):\n",
        "        PrefixTree.add_all(root, np.array(permutations_chunk), prefix_len, chunksize*i)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHYC8ybDFC9F"
      },
      "source": [
        "##Reordering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxNWYneQFGFc"
      },
      "source": [
        "def reorder_datastore_in_memory(input_file, output_file, indexing_mapping):\n",
        "  \"\"\"\n",
        "  Reorder the rows of a csv file (in memory) according to indexing_mapping \n",
        "  \"\"\"\n",
        "  # Read file\n",
        "  features = pd.read_csv(os.path.join(FEATURES_PATH,input_file), sep=',', header=None)\n",
        "  # Insert a new column containing the Indexing\n",
        "  features.insert(0, 'Indexing', indexing_mapping)\n",
        "  # Sort the rows by the Indexing \n",
        "  features = features.sort_values(by=['Indexing']).drop(labels = 'Indexing', axis=1)  \n",
        "  # Save reordered rows in new file\n",
        "  features.to_csv(os.path.join(BASE_PP_INDEX_PATH,output_file), mode='w', index = False, header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIkEbWmrFHTp"
      },
      "source": [
        "  \"\"\"\n",
        "  Reorder the rows of a csv file (on disk) according to leaves_list\n",
        "  \"\"\"\n",
        "  # Delete old file\n",
        "  if os.path.isfile(os.path.join(BASE_PP_INDEX_PATH,output_file)):\n",
        "    os.remove(os.path.join(BASE_PP_INDEX_PATH,output_file))\n",
        "\n",
        "  # For each leaf in the list\n",
        "  for leaf in tqdm(leaves_list):\n",
        "\n",
        "    # Read a row and save it to output_file (append mode)\n",
        "    df_features = pd.read_csv(os.path.join(FEATURES_PATH,input_file), sep=',', header=None, skiprows= leaf, nrows=1).to_csv(os.path.join(BASE_PP_INDEX_PATH,output_file), mode='a', index = False, header=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pDqwwjrA5AU"
      },
      "source": [
        "##PP-Index Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OEOv6FTBBB7"
      },
      "source": [
        "def create_PP_index(n_pivots, prefix_len, features_file, pivots_file, permutations_file, reordered_features_file, labels_file, reordered_labels_file, k_medoids = False, aproximation_size = 25000, chunksize = 1000, on_disk = False, num_indexes = 1):\n",
        "  '''\n",
        "  Creates the PP Index and returns the prefix tree\n",
        "  Pass only file name without extention\n",
        "  '''\n",
        "\n",
        "  prefix_trees = []\n",
        "\n",
        "  for idx in range(num_indexes):\n",
        "    # Pivot extraction from features space\n",
        "    if k_medoids:\n",
        "      extract_k_medoids_pivots(features_file + \".csv\",pivots_file + \"_\" + str(idx) + \".csv\", aproximation_size, n_pivots)\n",
        "    else:\n",
        "      extract_random_pivots(features_file + \".csv\", pivots_file + \"_\" + str(idx) + \".csv\", n_pivots)\n",
        "\n",
        "    # Dataset of images permutations of indexes \n",
        "    create_permutation_dataset(features_file + \".csv\", permutations_file + \"_\" + str(idx) + \".csv\", pivots_file + \"_\" + str(idx) + \".csv\", chunksize) \n",
        "\n",
        "    # Creates Empty Prefix Tree\n",
        "    prefix_tree_root = PrefixTree(\"*\")\n",
        "    # Adds all images pivots permutations in \"permutations.csv\"\n",
        "    PrefixTree.add_from_csv(prefix_tree_root, permutations_file + \"_\" + str(idx) + \".csv\", prefix_len, chunksize = chunksize)\n",
        "\n",
        "    # Get images indexes ordered by the prefix tree\n",
        "    indexes_reordered_by_prefix_tree = PrefixTree.get_leaves(prefix_tree_root)\n",
        "    print(indexes_reordered_by_prefix_tree)\n",
        "\n",
        "    if on_disk:\n",
        "      # On Disk features and labels reordering (Slow using colab and google drive)\n",
        "      reorder_datastore_on_disk(features_file + \".csv\", reordered_features_file + \"_\" + str(idx) + \".csv\", indexes_reordered_by_prefix_tree)\n",
        "      reorder_datastore_on_disk(labels_file + \".csv\", reordered_labels_file + \"_\" + str(idx) + \".csv\", indexes_reordered_by_prefix_tree)\n",
        "    else:\n",
        "      # Reorder indexes to obtain a mapping necessary to quick memory sort\n",
        "      indexing_mapping = [indexes_reordered_by_prefix_tree.index(i) for i in tqdm(range(len(indexes_reordered_by_prefix_tree)))]\n",
        "      print(indexing_mapping)\n",
        "\n",
        "      # In memory features and labels reordering\n",
        "      reorder_datastore_in_memory(features_file + \".csv\", reordered_features_file + \"_\" + str(idx) + \".csv\", indexing_mapping)\n",
        "      reorder_datastore_in_memory(labels_file + \".csv\", reordered_labels_file + \"_\" + str(idx) + \".csv\", indexing_mapping)\n",
        "\n",
        "    # Merge leaves in order to have only start and end index for a certain permutation\n",
        "    PrefixTree.merge_leaves(prefix_tree_root)\n",
        "    #PrefixTree.print_tree(prefix_tree_root)\n",
        "    prefix_trees.append(prefix_tree_root)\n",
        "\n",
        "  return prefix_trees"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcOgOW6hF0s-"
      },
      "source": [
        "##Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EumK1Zwh2mO"
      },
      "source": [
        "def extract_features(feature_extractor, images):\n",
        "  \"\"\"\n",
        "  Extract features from a given image\n",
        "  \"\"\"\n",
        "  # Extract features\n",
        "  features = feature_extractor.predict(images, batch_size=BATCH_SIZE)\n",
        "  # Rescale between 0 and 1\n",
        "  features = minmax_scale(features,axis=1)\n",
        "  # Add a dummy element in position 0 (for compatibility with other functions)\n",
        "  features = np.column_stack([[[0]], features])\n",
        "\n",
        "  return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5dtvNZSGAPc"
      },
      "source": [
        "def search_candidates(query_features, prefix_tree, pivots, Z, features_file, labels_file, precomputed_query_prefix = None):\n",
        "  \"\"\"\n",
        "  Explore the prefix tree to find candidates for a query\n",
        "  \"\"\"\n",
        "  # If the query prefix is not given, compute \n",
        "  if precomputed_query_prefix is None:\n",
        "    query_prefix = compute_permutation_prefix(query_features, PREFIX_LEN, pivots)\n",
        "  else:\n",
        "    query_prefix = precomputed_query_prefix\n",
        "  \n",
        "  # Find the smallest subtree containing at least Z elements in the leaves\n",
        "  subtree = PrefixTree.find_prefix(prefix_tree, query_prefix, Z)\n",
        "  # PrefixTree.print_tree(subtree)\n",
        "  leaves = PrefixTree.get_leaves(subtree)\n",
        "\n",
        "  # Read features and the corrispondet labels according to the subtree leaves\n",
        "  df_features = pd.read_csv(os.path.join(BASE_PP_INDEX_PATH,features_file), sep=',', header=None, skiprows= leaves[0], nrows=(leaves[-1] - leaves[0] +1)).to_numpy()\n",
        "  df_labels = pd.read_csv(os.path.join(BASE_PP_INDEX_PATH,labels_file), sep=',', header=None, skiprows= leaves[0], nrows=(leaves[-1] - leaves[0] +1)).to_numpy().reshape(-1)\n",
        "\n",
        "  return df_features, df_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFy5MqgfIMHS"
      },
      "source": [
        "def k_nn_search(query_features, candidates, candidates_labels, k, from_zip = False, attachID = False):\n",
        "  '''\n",
        "  Search for k-nearest images to the query by features cosine_similarity\n",
        "  Returns paths, similarities, labels\n",
        "  '''\n",
        "  similarity_list =[]\n",
        "\n",
        "\n",
        "  # Computes cosine similarity for every candidate\n",
        "  print(f\"\\nCandidates: {len(candidates)}\")\n",
        "  for candidate in candidates:\n",
        "    dist = cosine_similarity(np.array(query_features[1:]).reshape(1,-1),np.array(candidate[1:]).reshape(1,-1))[0]\n",
        "    similarity_list.extend(dist)\n",
        "\n",
        "  # Reorder the similarity list \n",
        "  similarities_from_pivots = np.array(similarity_list)\n",
        "  sorted_list = np.argsort(similarities_from_pivots)[::-1] #reverse\n",
        "\n",
        "  paths = []\n",
        "  BASE = \"https://drive.google.com/uc?export=view&id=\"\n",
        "\n",
        "  for idx in sorted_list[:k]:\n",
        "\n",
        "    c_path = candidates[idx,0]\n",
        "\n",
        "    # If we wanto to read from drive\n",
        "    if attachID:\n",
        "      # Extract images paths\n",
        "      id=xattr.getxattr(c_path, \"user.drive.id\")\n",
        "      c_path = BASE + str(id).split(\"'\")[1]\n",
        "    \n",
        "    # If we wanto to read from zip\n",
        "    if from_zip:\n",
        "      c_path.replace('/content/gdrive/MyDrive/Data/mirflickr25k', '/content')\n",
        "      c_path.replace('/content/gdrive/MyDrive/Data/facial_expression', '/content')\n",
        "      paths.append(c_path)\n",
        "    else:\n",
        "      paths.append(c_path)\n",
        "\n",
        "  return paths, np.array(similarity_list)[sorted_list[:k]], np.array(candidates_labels)[sorted_list[:k]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_4xz51k1JX9"
      },
      "source": [
        "def search(queries_features, K, pivot_file, reordered_features_file, reordered_labels_file, from_zip = False, attachID = False, query_perturbation = False, perturbation_len = 3):\n",
        "  '''\n",
        "  Search for k best images similar to queries using PP-Index\n",
        "  Returns topk_images_list, topk_scores_list, topk_labels_list\n",
        "  '''\n",
        "\n",
        "  topk_images_list = []\n",
        "  topk_scores_list = []\n",
        "  topk_labels_list = []\n",
        "\n",
        "  for query in tqdm(queries_features): \n",
        "    \n",
        "    topk_to_be_merged = np.array([[\"dummy_path\", \"-1\", \"dummy_label\"]])\n",
        "\n",
        "    # Merge all results from every PP Index if multiple are used\n",
        "    for idx, prefix_tree in enumerate(PREFIX_TREES):\n",
        "\n",
        "      # File names handling\n",
        "      pivots = np.array(pd.read_csv(os.path.join(BASE_PP_INDEX_PATH, pivot_file + \"_\" + str(idx) + \".csv\"), sep=',',header=None))\n",
        "      reordered_features = reordered_features_file + \"_\" + str(idx) + \".csv\"\n",
        "      reordered_labels = reordered_labels_file + \"_\" + str(idx) + \".csv\"\n",
        "\n",
        "      # If query perturbation is used\n",
        "      if query_perturbation:\n",
        "\n",
        "        # Compute all permutations of the first perturbation_len values of the query prefix\n",
        "        query_prefix = compute_permutation_prefix(query, PREFIX_LEN, pivots)\n",
        "        to_be_perturbated = query_prefix[:perturbation_len]\n",
        "        permutations = list(itool.permutations(to_be_perturbated))\n",
        "\n",
        "        # Search candidates for the first permutation. Save the candidates together with their labels\n",
        "        query_prefix[:perturbation_len] = np.array(permutations[0])\n",
        "        new_candidates, candidates_labels = search_candidates(query, prefix_tree, pivots, N_CANDIDATES, reordered_features, reordered_labels, query_prefix)\n",
        "        candidates = np.column_stack([np.array(candidates_labels).reshape(-1,1), np.array(new_candidates)])\n",
        "\n",
        "        # Search candidates for all the other permutations. Save the candidates together with their labels\n",
        "        for permutation in permutations[1:]:\n",
        "\n",
        "          query_prefix[:perturbation_len] = np.array(permutation)\n",
        "          new_candidates, candidates_labels = search_candidates(query, prefix_tree, pivots, N_CANDIDATES, reordered_features, reordered_labels, query_prefix)\n",
        "          # Merge candidates for all the permutations\n",
        "          candidates = np.concatenate((np.column_stack([np.array(candidates_labels).reshape(-1,1), np.array(new_candidates)]), candidates))\n",
        "\n",
        "        # Remove dupliacates and separate candidates labels from candidates\n",
        "        unique_candidates = np.unique(candidates.astype(\"str\"), axis=0)\n",
        "        candidates_labels = unique_candidates[:,0].astype('int') \n",
        "        candidates = unique_candidates[:,1:]\n",
        "\n",
        "      # Query perturbation is not used\n",
        "      else:\n",
        "        candidates, candidates_labels = search_candidates(query, prefix_tree, pivots, N_CANDIDATES, reordered_features, reordered_labels)\n",
        "\n",
        "      # k_nn_search to obtain topk_paths, topk_scores, topk_lables\n",
        "      topk_paths,  topk_scores, topk_labels= k_nn_search(query, candidates, candidates_labels, K, from_zip, attachID)\n",
        "\n",
        "      # Concatenate top k results obtained from each index\n",
        "      topk_to_be_merged = np.concatenate((np.array(list(zip(topk_paths,topk_scores,topk_labels))), topk_to_be_merged))\n",
        "    \n",
        "    # Remove duplicates and sort by score\n",
        "    unique_topk = np.unique(topk_to_be_merged.astype(\"str\"), axis=0)\n",
        "    unique_topk = unique_topk[unique_topk[:,1].argsort()][::-1]\n",
        "\n",
        "    # Take top k results\n",
        "    topk_paths = unique_topk[:K,0]\n",
        "    topk_scores = unique_topk[:K,1].astype('float64') \n",
        "    topk_labels = unique_topk[:K,2].astype('int')\n",
        "\n",
        "    # If we read from drive\n",
        "    if not attachID:\n",
        "      topk_images = [img_to_array(load_img(path, target_size=(PREVIEW_SIZE,PREVIEW_SIZE))) for path in  topk_paths]\n",
        "      topk_images_list.append(topk_images)\n",
        "    else:\n",
        "      topk_images_list.append(topk_paths)    \n",
        "\n",
        "    topk_scores_list.append(topk_scores)\n",
        "    topk_labels_list.append(topk_labels)\n",
        "\n",
        "  return topk_images_list, topk_scores_list, topk_labels_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKMVHKWT7CMO"
      },
      "source": [
        "##Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fYp_kKn7Lcf"
      },
      "source": [
        "# utility to resize, pad, and write score on images\n",
        "def process_images(image, score, is_relevant):\n",
        "  image = img_to_array(image)\n",
        "\n",
        "  # to uint8\n",
        "  # image = ((image + 1) * 127.5).astype(np.uint8)\n",
        "  image = image.astype(np.uint8)\n",
        "\n",
        "  # add a red/green flag\n",
        "  color = (0, 255, 0) if is_relevant else (255, 0, 0)\n",
        "  flag = np.full((10, PREVIEW_SIZE, 3), fill_value=color, dtype=image.dtype)\n",
        "  image = np.concatenate((image, flag), axis=0)\n",
        "\n",
        "  # resize\n",
        "  image = Image.fromarray(image).convert('RGBA')\n",
        "  image.thumbnail((PREVIEW_SIZE, PREVIEW_SIZE))  # use PIL to resize the image\n",
        "\n",
        "  # draw score\n",
        "  draw = ImageDraw.Draw(image)\n",
        "  draw.text((3, PREVIEW_SIZE - 12), f'{score:.2f}', anchor='lt', fill=(255, 255, 255, 255))\n",
        "\n",
        "  # pad the image with transparency\n",
        "  image = ImageOps.expand(image, 5, fill=(0, 0, 0, 0))\n",
        "  image = np.array(image)\n",
        "  return image\n",
        "\n",
        "# use np.vectorize to apply custom functions to numpy arrays\n",
        "np_process_image = np.vectorize(process_images, signature='(h,w,c),(),()->(h1,w1,c1)')\n",
        "\n",
        "# utility function to draw knn results for multiple queries\n",
        "def show_results(images, scores, is_relevant):\n",
        "  # images has shape (n_queries, k, H, W, C)\n",
        "  images = np_process_image(images, scores, is_relevant)\n",
        "  images = np.concatenate(images, axis=1)  # concatenate queries vertically\n",
        "  images = np.concatenate(images, axis=1)  # concatenate results horizontally\n",
        "  display(Image.fromarray(images))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSjTKNm57Md1"
      },
      "source": [
        "def evaluate_knn_search(queries_features, queries_labels, k, pivot_file, reordered_features_file, reordered_labels_file, from_zip=False, query_perturbation=False):\n",
        "  '''\n",
        "  Evaluate APs and mean AP of all queries\n",
        "  '''\n",
        "\n",
        "  # Search \n",
        "  topk_images, topk_scores, topk_labels = search(queries_features, k, pivot_file, reordered_features_file, reordered_labels_file, from_zip, query_perturbation = query_perturbation)\n",
        "  # If a certain retreived image is relevant \n",
        "  topk_is_relevant = np.array(topk_labels) == queries_labels.reshape(-1,1)\n",
        "\n",
        "  # Average Precision Score computation\n",
        "  aps = np.array([sklearn.metrics.average_precision_score(l, s) for l,s in zip(topk_is_relevant, topk_scores)])\n",
        "  aps[np.isnan(aps)] = 0\n",
        "  print('APs per Query:', aps)\n",
        "  print('mAP:', np.mean(aps))\n",
        "\n",
        "  # Shows a result grid\n",
        "  show_results(topk_images, topk_scores, topk_is_relevant)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGJN5iFZXS2d"
      },
      "source": [
        "#STEP 2: FEATURE EXTRACTION FROM FACIAL EXPRESSION DATASET AND MIRFLICKR25 USING FACENET\n",
        "In this step, features are extracted from both *Facial Expression Dataset* and *MirFlickr Dataset* using the pretrained network *Facenet*. Features are normalized using the *minmax_scale* ([0,1] interval). Files paths are written together with the extracted features in the *pretrained_features_normal.csv* file. Features from *private_test_set* are stored in a different file (*pretrained_features_private_normal.csv*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C6rlQFJkY6p"
      },
      "source": [
        "##Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhD48NWXHoSa",
        "outputId": "0edac680-b25b-4fcc-fa48-ba9ac387df51"
      },
      "source": [
        "\"\"\"\n",
        "FACENET MODEL LOADING\n",
        "Facenet model is loaded from the file facenet_keras.h5.\n",
        "\"\"\"\n",
        "\n",
        "MODEL_PATH='/content/gdrive/My Drive/Data/Facenet/model'\n",
        "WEIGHTS_PATH= '/content/gdrive/My Drive/Data/Facenet/weights'\n",
        "facenet = load_model(os.path.join(MODEL_PATH,'facenet_keras.h5'))\n",
        "facenet.load_weights(os.path.join(WEIGHTS_PATH,'facenet_keras_weights.h5'))\n",
        "facenet.trainable=False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1imGEWjfIh9_"
      },
      "source": [
        "\"\"\"\n",
        "FEATURES EXTRACTION FROM TRAINING SET\n",
        "Features are extracted and normalized using the minmax_scale function (they are\n",
        "translated into a value in [0,1] interval). File path is added as first column, \n",
        "so for each image the features file contains 129 values: path and 128 features \n",
        "(in the interval [0,1]).\n",
        "\"\"\"\n",
        "\n",
        "training_dataset_features = facenet.predict(training_dataset_noshuffle)\n",
        "training_dataset_features = minmax_scale(training_dataset_features,axis=1)\n",
        "training_dataset_noshuffle_path = np.array(training_dataset_noshuffle_path)\n",
        "training_dataset_features = np.column_stack([training_dataset_noshuffle_path,training_dataset_features])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2AoYQBJ5pqv"
      },
      "source": [
        "\"\"\"\n",
        "FEATURES EXTRACTION FROM PUBLIC AND TEST SET\n",
        "Features are extracted and normalized using the minmax_scale function (they are\n",
        "translated into a value in [0,1] interval). File path is added as first column, \n",
        "so for each image the features file contains 129 values: path and 128 features \n",
        "(in the interval [0,1]).\n",
        "\"\"\"\n",
        "\n",
        "public_test_dataset_features = facenet.predict(public_test_dataset)\n",
        "public_test_dataset_features = minmax_scale(public_test_dataset_features,axis=1)\n",
        "private_test_dataset_features = facenet.predict(private_test_dataset)\n",
        "private_test_dataset_features = minmax_scale(private_test_dataset_features,axis=1)\n",
        "\n",
        "public_test_dataset_path = np.array(public_test_dataset_path)\n",
        "private_test_dataset_path = np.array(private_test_dataset_path)\n",
        "\n",
        "public_test_dataset_features = np.column_stack([public_test_dataset_path,public_test_dataset_features])\n",
        "private_test_dataset_features = np.column_stack([private_test_dataset_path,private_test_dataset_features])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYnohmZRvasm"
      },
      "source": [
        "\"\"\"\n",
        "FEATURES EXTRACTION FROM MIRFLICKR DATASET\n",
        "Features are extracted and normalized using the minmax_scale function (they are\n",
        "translated into a value in [0,1] interval). File path is added as first column, \n",
        "so for each image the features file contains 129 values: path and 128 features \n",
        "(in the interval [0,1]).\n",
        "\"\"\"\n",
        "\n",
        "mirflickr_dataset_features = facenet.predict(mirflickr_dataset)\n",
        "mirflickr_dataset_features = minmax_scale(mirflickr_dataset_features,axis=1)\n",
        "mirflickr_dataset_path = np.array(mirflickr_dataset_path)\n",
        "mirflickr_dataset_features = np.column_stack([mirflickr_dataset_path,mirflickr_dataset_features])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bjv2iNEjkfxN"
      },
      "source": [
        "##Features Saving"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVWI6NVzmkeV"
      },
      "source": [
        "\"\"\"\n",
        "WRITE TRAINING SET FEATURES IN CSV FILE\n",
        "Training set features are written into the file pretrained_features_normal.csv\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.join(FEATURES_PATH, \"pretrained_features_normal.csv\"),\"ab\") as f:\n",
        "\n",
        "  np.savetxt(f, training_dataset_features, delimiter=\",\", fmt=\"%s\")\n",
        "\n",
        "\"\"\"\n",
        "WRITE PUBLIC TEST SET FEATURES IN CSV FILE\n",
        "Public test set features are written into the file pretrained_features_normal.csv\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.join(FEATURES_PATH, \"pretrained_features_normal.csv\"),\"ab\") as f:\n",
        "\n",
        "  np.savetxt(f, public_test_dataset_features, delimiter=\",\", fmt=\"%s\")\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "WRITE PRIVATE TEST SET FEATURES IN CSV FILE\n",
        "Private test set features are written into the file pretrained_features_private_normal.csv\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.join(FEATURES_PATH, \"pretrained_features_private_normal.csv\"),\"ab\") as f:\n",
        "\n",
        "  np.savetxt(f, private_test_dataset_features, delimiter=\",\", fmt=\"%s\")\n",
        "\n",
        "\"\"\"\n",
        "WRITE MIRFLICKR SET FEATURES IN CSV FILE\n",
        "Mirflickr set features are written into the file pretrained_features_normal.csv\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.join(FEATURES_PATH, \"pretrained_features_normal.csv\"),\"ab\") as f:\n",
        "\n",
        "  np.savetxt(f, mirflickr_dataset_features, delimiter=\",\", fmt=\"%s\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s-fbNx9Z4cj"
      },
      "source": [
        "#STEP 3: INDEX THE EXTRACTED FEATURES USING PP-INDEX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XrdNxP0zBJW"
      },
      "source": [
        "BASE_PP_INDEX_PATH = \"/content/gdrive/My Drive/Data/PP-Index\"\n",
        "FEATURES_PATH = \"/content/gdrive/My Drive/Data/features\"\n",
        "N_PIVOTS = 10\n",
        "PREFIX_LEN = 4\n",
        "CHUNKSIZE = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6H56258KLK7m"
      },
      "source": [
        "# Pivot extraction from features space\n",
        "pretrained_tree = create_PP_index(N_PIVOTS, PREFIX_LEN, \"pretrained_features\", \"pretrained_pivots\", \"pretrained_permutations\", \"pretrained_reordered_features\", \"labels\", \"pretrained_reordered_labels\", k_medoids = False, chunksize = 1000, on_disk = False, num_indexes = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orS5AlWkcs19"
      },
      "source": [
        "#STEP 4: MEASURE THE RETRIEVAL PERFORMANCE OF THE NEW IMAGE SEARCH ENGINE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d2SRXRa7cbp"
      },
      "source": [
        "N_QUERIES = 7\n",
        "K = 100\n",
        "N_CANDIDATES = 300\n",
        "PREVIEW_SIZE = 100\n",
        "\n",
        "PREFIX_TREES = pretrained_tree\n",
        "PIVOTS = \"pretrained_pivots\"\n",
        "REORDERED_FEATURES = \"pretrained_reordered_features\"\n",
        "REORDERED_LABELS = \"pretrained_reordered_labels\"\n",
        "\n",
        "test_Labels = np.array(pd.read_csv(os.path.join(FEATURES_PATH, \"private_test_labels.csv\"), sep=',',header=None)).reshape(-1)\n",
        "\n",
        "# One for each class\n",
        "query_indexes = [100, 500,  700, 1200,  2200, 2900, 3400]\n",
        "\n",
        "# Random\n",
        "# query_indexes = np.random.choice(len(test_features), N_QUERIES, replace=False)  # get n queries at random\n",
        "\n",
        "queries_labels = test_Labels[query_indexes]\n",
        "test_features = np.array(pd.read_csv(os.path.join(FEATURES_PATH, \"pretrained_features_private.csv\"), sep=',',header=None))\n",
        "queries_features = test_features[query_indexes]\n",
        "\n",
        "for fetures, label in zip(queries_features, queries_labels):\n",
        "  print(CATEGORICAL_LABELS[label])\n",
        "  display(load_img(fetures[0], target_size=(PREVIEW_SIZE,PREVIEW_SIZE)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAYWwrNrdFV_"
      },
      "source": [
        "evaluate_knn_search(queries_features, queries_labels, K, PIVOTS, REORDERED_FEATURES, REORDERED_LABELS, from_zip=True, query_perturbation=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNzWNOjcu-AD"
      },
      "source": [
        "#STEP 5: FINE-TUNING FACENET FOR FACIAL EXPRESSION DATASET\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwYJHhEVFWqG",
        "outputId": "49f0a93a-59d4-4bd8-fd07-c2caa7f8ab4e"
      },
      "source": [
        "\"\"\"\n",
        "FACENET MODEL LOADING\n",
        "Facenet model is loaded. The base is frozen so that the weights are not modified \n",
        "during the first training phase.\n",
        "\"\"\"\n",
        "\n",
        "MODEL_PATH='/content/gdrive/My Drive/Data/Facenet/model'\n",
        "WEIGHTS_PATH= '/content/gdrive/My Drive/Data/Facenet/weights'\n",
        "facenet = load_model(os.path.join(MODEL_PATH,'facenet_keras.h5'))\n",
        "facenet.load_weights(os.path.join(WEIGHTS_PATH,'facenet_keras_weights.h5'))\n",
        "facenet.trainable=False"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRWJBTBWxAsD",
        "outputId": "0488066a-2c57-40b6-b29b-78d7ab054b4f"
      },
      "source": [
        "print(facenet.inputs)\n",
        "print(facenet.outputs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<KerasTensor: shape=(None, 160, 160, 3) dtype=float32 (created by layer 'input_1')>]\n",
            "[<KerasTensor: shape=(None, 128) dtype=float32 (created by layer 'Bottleneck_BatchNorm')>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kawvwv5F0v-1"
      },
      "source": [
        "\"\"\"\n",
        "MODEL CREATION\n",
        "A fully connected layer is added on the top of Facenet in order to perform the\n",
        "classification. Softmax is used as activation function. The number of units is \n",
        "equal to the number of classes.\n",
        "\"\"\"\n",
        "\n",
        "model=Sequential()\n",
        "model.add(facenet)\n",
        "model.add(Dense(7,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzoeCHDxJxmB"
      },
      "source": [
        "\"\"\"\n",
        "MODEL COMPILATION\n",
        "For the first training phase, sparse_categorical_crossentropy is used as loss function\n",
        "and Adam (with LR=1e-3) as optimizer. \n",
        "\"\"\"\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=Adam(learning_rate=1e-3),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dbs7LTip_p2g"
      },
      "source": [
        "##Training the classifier on the top of Facenet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoJHEPTD8KlB"
      },
      "source": [
        "Since class distribution is unbalanced, different weights are assigned to different classes according to the formula used in [Tensorflow Tutorial](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#calculate_class_weights):\n",
        "\n",
        "\\begin{equation}\n",
        "weight_K= \\frac{T}{C*K}\n",
        "\\end{equation}\n",
        "\n",
        "where: <br>\n",
        "$T$ = total number of elements <br>\n",
        "$C$ = total number of classes <br>\n",
        "$K$ = number of elements of class K <br>\n",
        "$W_k$ = weight assigned to class k <br><br>\n",
        "The model is trained for 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGtnf4X6iDjb"
      },
      "source": [
        "\"\"\"\n",
        "ASSIGN WEIGHTS FOR EACH CLASS\n",
        "Weights are assigned to the different classes according to the formula defined\n",
        "above.\n",
        "\"\"\"\n",
        "\n",
        "train_labels=np.concatenate([y for x, y in training_dataset_noshuffle],axis=0)\n",
        "weight_for_0=1/(np.count_nonzero(train_labels==0))*train_labels.size/7.0\n",
        "weight_for_1=1/(np.count_nonzero(train_labels==1))*train_labels.size/7.0\n",
        "weight_for_2=1/(np.count_nonzero(train_labels==2))*train_labels.size/7.0\n",
        "weight_for_3=1/(np.count_nonzero(train_labels==3))*train_labels.size/7.0\n",
        "weight_for_4=1/(np.count_nonzero(train_labels==4))*train_labels.size/7.0\n",
        "weight_for_5=1/(np.count_nonzero(train_labels==5))*train_labels.size/7.0\n",
        "weight_for_6=1/(np.count_nonzero(train_labels==6))*train_labels.size/7.0\n",
        "class_weight = {0: weight_for_0, 1: weight_for_1, 2: weight_for_2, 3: weight_for_3, 4: weight_for_4, 5: weight_for_5, 6: weight_for_6}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTC5coxR-dli",
        "outputId": "327f8798-811b-44e7-86ec-be12f0a90384"
      },
      "source": [
        "print(class_weight)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 1.0266046844269623, 1: 9.406618610747051, 2: 1.0010460615781582, 3: 0.5684387684387684, 4: 0.8260394187886636, 5: 0.8491274770777876, 6: 1.293372978330405}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GKS04xNKU6Y"
      },
      "source": [
        "\"\"\"\n",
        "MODEL FITTING\n",
        "Model is trained for a limited number of epochs (10). Batch size is equal to 64.\n",
        "\"\"\"\n",
        "\n",
        "history = model.fit(\n",
        "      train_generator,\n",
        "      batch_size = BATCH_SIZE,\n",
        "      epochs=10,\n",
        "      validation_data = validation_generator,\n",
        "      class_weight=class_weight\n",
        "      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaBTnUedcTE-"
      },
      "source": [
        "##Evaluate the performances of classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cR8mtDVcMvt5"
      },
      "source": [
        "\"\"\"\n",
        "PLOT TRAINING AND VALIDATION LOSS AND ACCURACY  \n",
        "\"\"\"\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'g', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'g', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LQPulL_c5z3"
      },
      "source": [
        "\"\"\"\n",
        "GETTING ACCURACY AND OTHER METRICS ON PUBLIC TEST DATASET\n",
        "In order to obtain the metrics, labels predicted from the model are compared with\n",
        "the original ones (from public_test_dataset). \n",
        "\"\"\"\n",
        "\n",
        "test_labels=np.concatenate([y for x, y in public_test_dataset], axis=0)\n",
        "test_predictions = model.predict(public_test_dataset)\n",
        "pred_labels=np.argmax(test_predictions, axis=-1)\n",
        "print(sklearn.metrics.classification_report(test_labels, pred_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1xqTRaZCpT9"
      },
      "source": [
        "\"\"\"\n",
        "SAVE BASE MODEL\n",
        "The model is saved into the file base_classifier02.h5.\n",
        "\"\"\"\n",
        "\n",
        "model.save(os.path.join(MODELS_PATH,\"base_classifier02.h5\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wpkc5oFVcXyi"
      },
      "source": [
        "##Fine Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U3hPMCxa7NZ"
      },
      "source": [
        "\"\"\"\n",
        "UNFREEZING ALL BASE LAYERS\n",
        "For the second training step, the whole Facenet base is unfrozen, so that all weights\n",
        "are modified in this phase.\n",
        "\"\"\"\n",
        "\n",
        "facenet.trainable = True\n",
        "model=Sequential()\n",
        "model.add(facenet)\n",
        "model.add(Dense(7,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tmRZaSccQdj"
      },
      "source": [
        "\"\"\"\n",
        "MODEL COMPILATION\n",
        "For this phase, sparse_categorical_crossentropy is used as loss function, while \n",
        "the optimizer is RMSprop (with LR=1e-5) because of its higher speed.\n",
        "\"\"\"\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=RMSprop(lr=1e-5),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ziduzh2lHT7"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6LIZqyhkaVN"
      },
      "source": [
        "\"\"\"\n",
        "MODEL FITTING\n",
        "Batch size is still 64. The maximum number of epochs is 100, however a Callback with \n",
        "patience 10 is used to prevent overfitting.\n",
        "\"\"\"\n",
        "\n",
        "history = model.fit(\n",
        "      train_generator,\n",
        "      batch_size = BATCH_SIZE,\n",
        "      epochs=100,\n",
        "      validation_data = validation_generator,\n",
        "      class_weight=class_weight,\n",
        "      callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience= 10)]\n",
        "      )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ-umuZSccLh"
      },
      "source": [
        "##Evaluate Fine Tuned Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5Zv1EmIk8Av"
      },
      "source": [
        "\"\"\"\n",
        "PLOT TRAINING AND VALIDATION LOSS AND ACCURACY \n",
        "\"\"\"\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'g', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'g', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDSAlEyok8Av"
      },
      "source": [
        "\"\"\"\n",
        "GETTING ACCURACY AND OTHER METRICS ON PUBLIC TEST DATASET\n",
        "In order to obtain the metrics, labels predicted from the model are compared with\n",
        "the original ones (from public_test_dataset).\n",
        "\"\"\"\n",
        "\n",
        "test_labels=np.concatenate([y for x, y in public_test_dataset], axis=0)\n",
        "test_predictions = model.predict(public_test_dataset)\n",
        "pred_labels=np.argmax(test_predictions, axis=-1)\n",
        "print(sklearn.metrics.classification_report(test_labels, pred_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbVxna1OCH2o"
      },
      "source": [
        "\"\"\"\n",
        "SAVE THE MODEL\n",
        "\"\"\"\n",
        "\n",
        "model.save(os.path.join(MODELS_PATH,\"fine_tuned_model002.h5\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S49lWD3Cm631"
      },
      "source": [
        "facenet.save(os.path.join(MODELS_PATH,\"facenet_ft2.h5\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXvPVaInbFNP"
      },
      "source": [
        "#STEP 6: USE THE FINE-TUNED FACENET TO EXTRACT FEATURES FROM FACIAL EXPRESSION DATASET AND MIRFLICKR\n",
        "In this step, features are extracted from both *Facial Expression Dataset* and *MirFlickr Dataset* using the fine-tuned network. Features are normalized using the *minmax_scale* ([0,1] interval). Files paths are written together with the extracted features in the *finetuned_features_normal.csv* file. Features from *private_test_set* are stored in a different file (*finetuned_features_private_normal.csv*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oyUO6THtUKf"
      },
      "source": [
        "##Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1CSGMyNdAG0"
      },
      "source": [
        "\"\"\"\n",
        "TRAINING SET FEATURE EXTRACTION\n",
        "Features are extracted and normalized using the minmax_scale function (they are\n",
        "translated into a value in [0,1] interval). File path is added as first column, \n",
        "so for each image the features file contains 129 values: path and 128 features \n",
        "(in the interval [0,1]).\n",
        "\"\"\"\n",
        "\n",
        "training_dataset_features = facenet.predict(training_dataset_noshuffle)\n",
        "training_dataset_features = minmax_scale(training_dataset_features,axis=1)\n",
        "training_dataset_noshuffle_path = np.array(training_dataset_noshuffle_path)\n",
        "training_dataset_features = np.column_stack([training_dataset_noshuffle_path,training_dataset_features])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CiQgi9rJIoy"
      },
      "source": [
        "\"\"\"\n",
        "PUBLIC AND PRIVATE TEST SET FEATURE EXTRACTION\n",
        "Features are extracted and normalized using the minmax_scale function (they are\n",
        "translated into a value in [0,1] interval). File path is added as first column, \n",
        "so for each image the features file contains 129 values: path and 128 features \n",
        "(in the interval [0,1]).\n",
        "\"\"\"\n",
        "\n",
        "public_test_dataset_features = facenet.predict(public_test_dataset)\n",
        "private_test_dataset_features = facenet.predict(private_test_dataset)\n",
        "\n",
        "public_test_dataset_features = minmax_scale(public_test_dataset_features,axis=1)\n",
        "public_test_dataset_path = np.array(public_test_dataset_path)\n",
        "private_test_dataset_features = minmax_scale(private_test_dataset_features,axis=1)\n",
        "private_test_dataset_path = np.array(private_test_dataset_path)\n",
        "\n",
        "public_test_dataset_features = np.column_stack([public_test_dataset_path,public_test_dataset_features])\n",
        "private_test_dataset_features = np.column_stack([private_test_dataset_path,private_test_dataset_features])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQ1WPdb6dAG1"
      },
      "source": [
        "\"\"\"\n",
        "MIRFLICKR DATASET FEATURE EXTRACTION\n",
        "Features are extracted and normalized using the minmax_scale function (they are\n",
        "translated into a value in [0,1] interval). File path is added as first column, \n",
        "so for each image the features file contains 129 values: path and 128 features \n",
        "(in the interval [0,1]).\n",
        "\"\"\"\n",
        "\n",
        "mirflickr_dataset_features = facenet.predict(mirflickr_dataset)\n",
        "mirflickr_dataset_features = minmax_scale(mirflickr_dataset_features,axis=1)\n",
        "mirflickr_dataset_path = np.array(mirflickr_dataset_path)\n",
        "mirflickr_dataset_features = np.column_stack([mirflickr_dataset_path,mirflickr_dataset_features])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOBWv7iatax_"
      },
      "source": [
        "##Features Saving"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O21wu5P5dAG2"
      },
      "source": [
        "\"\"\"\n",
        "WRITE TRAINING SET FEATURES IN CSV FILE\n",
        "Training set features are written into the file finetuned_features_normal.csv\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.join(FEATURES_PATH, \"finetuned_features_normal.csv\"),\"ab\") as f:\n",
        "\n",
        "  np.savetxt(f, training_dataset_features, delimiter=\",\", fmt=\"%s\")\n",
        "\n",
        "\"\"\"\n",
        "WRITE PUBLIC TEST SET FEATURES IN CSV FILE\n",
        "Public test set features are written into the file finetuned_features_normal.csv\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.join(FEATURES_PATH, \"finetuned_features_normal.csv\"),\"ab\") as f:\n",
        "\n",
        "  np.savetxt(f, public_test_dataset_features, delimiter=\",\", fmt=\"%s\")\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "WRITE PRIVATE TEST SET FEATURES IN CSV FILE\n",
        "Private test set features are written into the file finetuned_features_private_normal.csv\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.join(FEATURES_PATH, \"finetuned_features_private_normal.csv\"),\"ab\") as f:\n",
        "\n",
        "  np.savetxt(f, private_test_dataset_features, delimiter=\",\", fmt=\"%s\")\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "WRITE MIRFLICKR SET FEATURES IN CSV FILE\n",
        "Mirflickr set features are written into the file finetuned_features_normal.csv\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.join(FEATURES_PATH, \"finetuned_features_normal.csv\"),\"ab\") as f:\n",
        "\n",
        "  np.savetxt(f, mirflickr_dataset_features, delimiter=\",\", fmt=\"%s\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz_S-_orbajq"
      },
      "source": [
        "#STEP 7: INDEX THE NEW EXTRACTED FEATURES USING PP-INDEX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsYAEJjq5mK5"
      },
      "source": [
        "BASE_PP_INDEX_PATH = \"/content/gdrive/My Drive/Data/PP-Index\"\n",
        "FEATURES_PATH = \"/content/gdrive/My Drive/Data/features\"\n",
        "N_PIVOTS = 7\n",
        "PREFIX_LEN = 4\n",
        "CHUNKSIZE = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFRs3hJX5mK6"
      },
      "source": [
        "# Pivot extraction from features space\n",
        "finetuned_tree = create_PP_index(N_PIVOTS, PREFIX_LEN, \"finetuned_features\", \"finetuned_pivots\", \"finetuned_permutations\", \"finetuned_reordered_features\", \"labels\", \"finetuned_reordered_labels\", k_medoids = True, chunksize = 1000, on_disk = False, num_indexes = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ubbhso6vbozk"
      },
      "source": [
        "#STEP 8: MEASURE THE RETRIEVAL PERFORMANCE OF THE NEW IMAGE SEARCH ENGINE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAbLm9-JK4IE"
      },
      "source": [
        "Qua c'era un errore perchè in realtà prendevate alcune cose dell'indexing pretrained (ad esempio pivots, features del private dataset)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqBIgaJ68Oo4"
      },
      "source": [
        "N_QUERIES = 7\n",
        "K = 100\n",
        "N_CANDIDATES = 300\n",
        "PREVIEW_SIZE = 100\n",
        "\n",
        "PREFIX_TREES = finetuned_tree\n",
        "PIVOTS = \"finetuned_pivots\"\n",
        "REORDERED_FEATURES = \"finetuned_reordered_features\"\n",
        "REORDERED_LABELS = \"finetuned_reordered_labels\"\n",
        "\n",
        "test_Labels = np.array(pd.read_csv(os.path.join(FEATURES_PATH, \"private_test_labels.csv\"), sep=',',header=None)).reshape(-1)\n",
        "\n",
        "# One for each class\n",
        "query_indexes = [100, 500,  700, 1200,  2200, 2900, 3400]\n",
        "\n",
        "# Random\n",
        "# query_indexes = np.random.choice(len(test_features), N_QUERIES, replace=False)  # get n queries at random\n",
        "\n",
        "queries_labels = test_Labels[query_indexes]\n",
        "test_features = np.array(pd.read_csv(os.path.join(FEATURES_PATH, \"finetuned_features_private.csv\"), sep=',',header=None))\n",
        "queries_features = test_features[query_indexes]\n",
        "\n",
        "for fetures, label in zip(queries_features, queries_labels):\n",
        "  print(CATEGORICAL_LABELS[label])\n",
        "  display(load_img(fetures[0], target_size=(PREVIEW_SIZE,PREVIEW_SIZE)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0y1kjoya8WfY"
      },
      "source": [
        "evaluate_knn_search(queries_features, queries_labels, K, PIVOTS, REORDERED_FEATURES, REORDERED_LABELS, from_zip=True, query_perturbation=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTdo7-48bsG4"
      },
      "source": [
        "#STEP 9: COMPARE THE PERFORMANCE OF THE TWO SEARCH ENGINES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7yrquyAt4nt"
      },
      "source": [
        "N_QUERIES = 7\n",
        "K = 100\n",
        "N_CANDIDATES = 300\n",
        "PREVIEW_SIZE = 100\n",
        "\n",
        "test_Labels = np.array(pd.read_csv(os.path.join(FEATURES_PATH, \"private_test_labels.csv\"), sep=',',header=None)).reshape(-1)\n",
        "\n",
        "# One for each class\n",
        "query_indexes = [100, 500,  700, 1200,  2200, 2900, 3400]\n",
        "\n",
        "# Random\n",
        "# query_indexes = np.random.choice(len(test_features), N_QUERIES, replace=False)  # get n queries at random\n",
        "\n",
        "queries_labels = test_Labels[query_indexes]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFM7AL_kATvV"
      },
      "source": [
        "BASE_PP_INDEX_PATH = \"/content/gdrive/My Drive/Data/PP-Index\"\n",
        "FEATURES_PATH = \"/content/gdrive/My Drive/Data/features\"\n",
        "N_PIVOTS = 7\n",
        "PREFIX_LEN = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFIbVQJWyK3k"
      },
      "source": [
        "test_features = np.array(pd.read_csv(os.path.join(FEATURES_PATH, \"pretrained_features_private.csv\"), sep=',',header=None))\n",
        "queries_features = test_features[query_indexes]\n",
        "\n",
        "for fetures, label in zip(queries_features, queries_labels):\n",
        "  print(CATEGORICAL_LABELS[label])\n",
        "  display(load_img(fetures[0], target_size=(PREVIEW_SIZE,PREVIEW_SIZE)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUapQTPqAbFe"
      },
      "source": [
        "# Pivot extraction from features space\n",
        "pretrained_tree = create_PP_index(N_PIVOTS, PREFIX_LEN, \"pretrained_features\", \"pretrained_pivots_compare\", \"pretrained_permutations_compare\", \"pretrained_reordered_features_compare\", \"labels\", \"pretrained_reordered_labels_compare\", k_medoids = True, chunksize = 1000, on_disk = False, num_indexes = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8CUyjKp2ual"
      },
      "source": [
        "PREFIX_TREES = pretrained_tree\n",
        "PIVOTS = \"pretrained_pivots_compare\"\n",
        "REORDERED_FEATURES = \"pretrained_reordered_features_compare\"\n",
        "REORDERED_LABELS = \"pretrained_reordered_labels_compare\"\n",
        "\n",
        "evaluate_knn_search(queries_features, queries_labels, K, PIVOTS, REORDERED_FEATURES, REORDERED_LABELS, from_zip=True, query_perturbation=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6MOzHg8Dddo"
      },
      "source": [
        "# Pivot extraction from features space\n",
        "finetuned_tree = create_PP_index(N_PIVOTS, PREFIX_LEN, \"finetuned_features\", \"finetuned_pivots_compare\", \"finetuned_permutations_compare\", \"finetuned_reordered_features_compare\", \"labels\", \"finetuned_reordered_labels_compare\", k_medoids = True, chunksize = 1000, on_disk = False, num_indexes = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4emNfb3uOgz"
      },
      "source": [
        "test_features = np.array(pd.read_csv(os.path.join(FEATURES_PATH, \"finetuned_features_private.csv\"), sep=',',header=None))\n",
        "queries_features = test_features[query_indexes]\n",
        "\n",
        "PREFIX_TREES = finetuned_tree\n",
        "PIVOTS = \"finetuned_pivots_compare\"\n",
        "REORDERED_FEATURES = \"finetuned_reordered_features_compare\"\n",
        "REORDERED_LABELS = \"finetuned_reordered_labels_compare\"\n",
        "\n",
        "evaluate_knn_search(queries_features, queries_labels, K, PIVOTS, REORDERED_FEATURES, REORDERED_LABELS, from_zip=True, query_perturbation=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8KycNp2KaZH"
      },
      "source": [
        "#STEP 10: FLASK WEB USER INTERFACE\n",
        "In order to create the Web App, *Flask* is employed. *Ngrok* is used to expose the server running on the virtual machine to a public URL, as described in [ngrok tutorial](https://medium.com/@kshitijvijay271199/flask-on-google-colab-f6525986797b#:~:text=Here%20comes%20the%20Python%20library%20flask%2Dngrok.&text=A%20secure%20URL%20is%20provided,before%20deploying%20it%20into%20production.)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrwnDK0nKaZH"
      },
      "source": [
        "\"\"\"\n",
        "MODEL LOADING AND VARIABLES DEFINITION\n",
        "Load the model that will be used to extract the features from the query image.\n",
        "\"\"\"\n",
        "\n",
        "model = load_model(os.path.join(MODELS_PATH, \"facenet_ft2.h5\"))\n",
        "\n",
        "K = 100\n",
        "N_CANDIDATES = 300\n",
        "BASE_PP_INDEX_PATH = \"/content/gdrive/My Drive/Data/PP-Index\"\n",
        "FEATURES_PATH = \"/content/gdrive/My Drive/Data/features\"\n",
        "PREFIX_LEN = 4\n",
        "CHUNKSIZE = 1000\n",
        "\n",
        "prefix_tree_web = PrefixTree(\"Prefix Tree WEB\")\n",
        "PrefixTree.add_from_csv(prefix_tree_web, \"finetuned_permutations_0.csv\", PREFIX_LEN, chunksize = CHUNKSIZE)\n",
        "PrefixTree.merge_leaves(prefix_tree_web)\n",
        "PREFIX_TREES = [prefix_tree_web]\n",
        "REORDERED_FEATURES = \"finetuned_reordered_features\"\n",
        "REORDERED_LABELS = \"finetuned_reordered_labels\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jlXyuhxAkpn"
      },
      "source": [
        "\"\"\"\n",
        "PREPROCESSING FUNCTION DEFINITION\n",
        "This function will be used to perform the preprocessing on the query image. \n",
        "The type of preprocessing used is exactly the same used in feature extraction phase.\n",
        "\"\"\"\n",
        "\n",
        "def preprocess(images, labels):\n",
        "  # rescales from [0, 255] to [-1, 1], equivalent to:  images = (images / 127.5) - 1\n",
        "  images = tf.keras.applications.mobilenet_v2.preprocess_input(images)\n",
        "  return images, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xsg8iFBLKaZI"
      },
      "source": [
        "\"\"\"\n",
        "WEB APP DEFINITION AND DEPLOYING\n",
        "In order to use the Web App, click on the second link.\n",
        "\"\"\"\n",
        "\n",
        "app = Flask(__name__,template_folder='/content/gdrive/MyDrive/templates',static_url_path='/content/gdrive/MyDrive/static')\n",
        "run_with_ngrok(app)\n",
        "app.config['UPLOAD_EXTENSIONS'] = ['.jpg', '.png', '.gif']\n",
        "results=[]\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    print(app.instance_path)\n",
        "    return render_template('index.html',results=results)\n",
        "\n",
        "@app.route('/search',methods=['POST'])\n",
        "def web_search():\n",
        "    uploaded_file = request.files['query']\n",
        "    filename = uploaded_file.filename\n",
        "\n",
        "    #remove all files from upload directory\n",
        "    files = glob.glob('/content/gdrive/MyDrive/static/upload/*')\n",
        "    for f in files:\n",
        "      os.remove(f)\n",
        "    \n",
        "    if filename != '':\n",
        "\n",
        "        #the uploaded file is accepted only if its extension is .jpg, .png or .gif\n",
        "        file_ext = os.path.splitext(filename)[1]\n",
        "        if file_ext not in app.config['UPLOAD_EXTENSIONS']:\n",
        "            abort(400)\n",
        "        \n",
        "        #save query file in upload directory, necessary to correctly reconstruct it into a tensor\n",
        "        file_path = os.path.join('/content/gdrive/MyDrive/static/upload', filename)\n",
        "        uploaded_file.save(file_path)\n",
        "\n",
        "        #read query image using image_dataset_from_directory, in order to perform the same preprocessing applied for feature extraction\n",
        "        queryds, query_path = image_dataset_from_directory(directory='/content/gdrive/MyDrive/static/', labels=\"inferred\", label_mode=\"int\", color_mode=\"rgb\", batch_size=BATCH_SIZE, image_size=(160,160), shuffle=False, seed = SEED)\n",
        "        queryds = queryds.map(preprocess, deterministic=True)\n",
        "\n",
        "        #extract features from query image using the specified model\n",
        "        query_features = extract_features(model, queryds)\n",
        "\n",
        "        #search for results \n",
        "        paths, scores, labels = search(query_features, K, pivot_file=\"finetuned_pivots\", reordered_features_file=REORDERED_FEATURES, reordered_labels_file=REORDERED_LABELS, attachID = True)\n",
        "\n",
        "        #transform the format so that it is understandable from html file\n",
        "        results = [ [x, np.round(y,3), CATEGORICAL_LABELS[z]] for x,y,z in zip(paths[0], scores[0], labels[0])]\n",
        "  \n",
        "    return render_template('index.html',results=results)\n",
        "\n",
        "if __name__== \"__main__\":\n",
        "  app.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHaUDCdWzT1U"
      },
      "source": [
        "#STEP 11: OPTIMIZATIONS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fwfrZ6nCGaR"
      },
      "source": [
        "## K-Medoids VS Random Pivots Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BEXXQLKG5e_"
      },
      "source": [
        "BASE_PP_INDEX_PATH = \"/content/gdrive/My Drive/Data/PP-Index\"\n",
        "FEATURES_PATH = \"/content/gdrive/My Drive/Data/features\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cmLBCTq4vkX"
      },
      "source": [
        "N_PIVOTS = 7\n",
        "PREFIX_LEN = 4\n",
        "PAM_root = create_PP_index(N_PIVOTS, PREFIX_LEN, \"finetuned_features\", \"finetuned_pivots_medoids\", \"finetuned_permutation_medoids\", \"finetuned_reordered_features_medoids\", \"labels\", \"finetuned_reordered_labels_medoids\", k_medoids = True, chunksize = 1000, on_disk = False, num_indexes = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLsE4RX7BvR3"
      },
      "source": [
        "N_PIVOTS = 10\n",
        "PREFIX_LEN = 4\n",
        "random_root = create_PP_index(N_PIVOTS, PREFIX_LEN,'finetuned_features', 'finetuned_pivots_random', \"finetuned_permutations_random\", 'reordered_finetuned_features_random','labels', 'reordered_finetuned_labels_random', chunksize = 1000, on_disk = False, num_indexes = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDA_wjH4B_5Y"
      },
      "source": [
        "N_QUERIES = 7\n",
        "K = 100\n",
        "N_CANDIDATES = 300\n",
        "PREVIEW_SIZE = 100\n",
        "\n",
        "test_Labels = np.array(pd.read_csv(os.path.join(FEATURES_PATH, \"private_test_labels.csv\"), sep=',',header=None)).reshape(-1)\n",
        "\n",
        "# One for each class\n",
        "query_indexes = [100, 500,  700, 1200,  2200, 2900, 3400]\n",
        "\n",
        "# Random\n",
        "# query_indexes = np.random.choice(len(test_features), N_QUERIES, replace=False)  # get n queries at random\n",
        "\n",
        "queries_labels = test_Labels[query_indexes]\n",
        "test_features = np.array(pd.read_csv(os.path.join(FEATURES_PATH, \"finetuned_features_private.csv\"), sep=',',header=None))\n",
        "queries_features = test_features[query_indexes]\n",
        "\n",
        "for fetures, label in zip(queries_features, queries_labels):\n",
        "  print(CATEGORICAL_LABELS[label])\n",
        "  display(load_img(fetures[0], target_size=(PREVIEW_SIZE,PREVIEW_SIZE)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJ2E7BzuCM00"
      },
      "source": [
        "PREFIX_TREES = PAM_root\n",
        "PIVOTS = \"finetuned_pivots_medoids\"\n",
        "REORDERED_FEATURES = \"finetuned_reordered_features_medoids\"\n",
        "REORDERED_LABELS = \"finetuned_reordered_labels_medoids\"\n",
        "\n",
        "evaluate_knn_search(queries_features, queries_labels, K, PIVOTS, REORDERED_FEATURES, REORDERED_LABELS, from_zip=True, query_perturbation=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtzpvJ1fDAbp"
      },
      "source": [
        "PREFIX_TREES = random_root\n",
        "PIVOTS = \"finetuned_pivots_random\"\n",
        "REORDERED_FEATURES = \"reordered_finetuned_features_random\"\n",
        "REORDERED_LABELS = \"reordered_finetuned_labels_random\"\n",
        "\n",
        "evaluate_knn_search(queries_features, queries_labels, K, PIVOTS, REORDERED_FEATURES, REORDERED_LABELS, from_zip=True, query_perturbation=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIAvBY8jYiIo"
      },
      "source": [
        "##Perturbations VS Non Perturbations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8fJOaNzH7XZ"
      },
      "source": [
        "BASE_PP_INDEX_PATH = \"/content/gdrive/My Drive/Data/PP-Index\"\n",
        "FEATURES_PATH = \"/content/gdrive/My Drive/Data/features\"\n",
        "N_PIVOTS = 7\n",
        "PREFIX_LEN = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCbMp-tXH7XZ"
      },
      "source": [
        "N_PIVOTS = 7\n",
        "PREFIX_LEN = 4\n",
        "pert_root = create_PP_index(N_PIVOTS, PREFIX_LEN, \"finetuned_features\", \"finetuned_pivots_pert\", \"finetuned_permutation_pert\", \"finetuned_reordered_features_pert\", \"labels\", \"finetuned_reordered_labels_pert\", k_medoids = True, chunksize = 1000, on_disk = False, num_indexes = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "li6Oh5-mH7Xa"
      },
      "source": [
        "N_QUERIES = 7\n",
        "K = 100\n",
        "N_CANDIDATES = 300\n",
        "PREVIEW_SIZE = 100\n",
        "PREFIX_TREES = pert_root\n",
        "PIVOTS = \"finetuned_pivots_pert\"\n",
        "REORDERED_FEATURES = \"finetuned_reordered_features_pert\"\n",
        "REORDERED_LABELS = \"finetuned_reordered_labels_pert\"\n",
        "\n",
        "test_Labels = np.array(pd.read_csv(os.path.join(FEATURES_PATH, \"private_test_labels.csv\"), sep=',',header=None)).reshape(-1)\n",
        "\n",
        "# One for each class\n",
        "query_indexes = [100, 500,  700, 1200,  2200, 2900, 3400]\n",
        "\n",
        "# Random\n",
        "# query_indexes = np.random.choice(len(test_features), N_QUERIES, replace=False)  # get n queries at random\n",
        "\n",
        "queries_labels = test_Labels[query_indexes]\n",
        "test_features = np.array(pd.read_csv(os.path.join(FEATURES_PATH, \"finetuned_features_private.csv\"), sep=',',header=None))\n",
        "queries_features = test_features[query_indexes]\n",
        "\n",
        "for fetures, label in zip(queries_features, queries_labels):\n",
        "  print(CATEGORICAL_LABELS[label])\n",
        "  display(load_img(fetures[0], target_size=(PREVIEW_SIZE,PREVIEW_SIZE)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-dsZR2DH7Xb"
      },
      "source": [
        "evaluate_knn_search(queries_features, queries_labels, K, PIVOTS, REORDERED_FEATURES, REORDERED_LABELS, from_zip=True, query_perturbation=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iM_Vm1wCH7Xb"
      },
      "source": [
        "evaluate_knn_search(queries_features, queries_labels, K, PIVOTS, REORDERED_FEATURES, REORDERED_LABELS, from_zip=True, query_perturbation=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHeBO3oz6_ui"
      },
      "source": [
        "##Multiple Indexes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PV-bQ2m67Tz"
      },
      "source": [
        "BASE_PP_INDEX_PATH = \"/content/gdrive/My Drive/Data/PP-Index\"\n",
        "FEATURES_PATH = \"/content/gdrive/My Drive/Data/features\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK6Jt3PWoA-P"
      },
      "source": [
        "N_PIVOTS = 10\n",
        "PREFIX_LEN = 4\n",
        "random_roots = create_PP_index(N_PIVOTS, PREFIX_LEN,'finetuned_features', 'finetuned_pivots_multiple_idexes', \"finetuned_permutations_multiple_idexes\", 'reordered_finetuned_features_multiple_idexes','labels', 'reordered_finetuned_labels_multiple_idexes', k_medoids = False, num_indexes=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UMaHoHPf6mK"
      },
      "source": [
        "N_PIVOTS = 7\n",
        "PREFIX_LEN = 4\n",
        "pam_roots = create_PP_index(N_PIVOTS, PREFIX_LEN,'finetuned_features', 'finetuned_pivots_multiple_idexes_medoids', \"finetuned_permutations_multiple_idexes_medoids\", 'reordered_finetuned_features_multiple_idexes_medoids','labels', 'reordered_finetuned_labels_multiple_idexes_medoids', k_medoids = True, num_indexes=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JI0STnE67TD0"
      },
      "source": [
        "N_QUERIES = 7\n",
        "K = 100\n",
        "N_CANDIDATES = 300\n",
        "PREVIEW_SIZE = 100\n",
        "\n",
        "test_Labels = np.array(pd.read_csv(os.path.join(FEATURES_PATH, \"private_test_labels.csv\"), sep=',',header=None)).reshape(-1)\n",
        "\n",
        "# One for each class\n",
        "query_indexes = [100, 500,  700, 1200,  2200, 2900, 3400]\n",
        "\n",
        "# Random\n",
        "# query_indexes = np.random.choice(len(test_features), N_QUERIES, replace=False)  # get n queries at random\n",
        "\n",
        "queries_labels = test_Labels[query_indexes]\n",
        "test_features = np.array(pd.read_csv(os.path.join(FEATURES_PATH, \"finetuned_features_private.csv\"), sep=',',header=None))\n",
        "queries_features = test_features[query_indexes]\n",
        "\n",
        "for fetures, label in zip(queries_features, queries_labels):\n",
        "  print(CATEGORICAL_LABELS[label])\n",
        "  display(load_img(fetures[0], target_size=(PREVIEW_SIZE,PREVIEW_SIZE)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBMVFFMr8D_F"
      },
      "source": [
        "PREFIX_TREES = random_roots\n",
        "PIVOTS = \"finetuned_pivots_multiple_idexes\"\n",
        "REORDERED_FEATURES = \"reordered_finetuned_features_multiple_idexes\"\n",
        "REORDERED_LABELS = \"reordered_finetuned_labels_multiple_idexes\"\n",
        "\n",
        "evaluate_knn_search(queries_features, queries_labels, K, PIVOTS, REORDERED_FEATURES, REORDERED_LABELS, from_zip=True, query_perturbation=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KWLec07hGBq"
      },
      "source": [
        "PREFIX_TREES = pam_roots\n",
        "PIVOTS = \"finetuned_pivots_multiple_idexes_medoids\"\n",
        "REORDERED_FEATURES = \"reordered_finetuned_features_multiple_idexes_medoids\"\n",
        "REORDERED_LABELS = \"reordered_finetuned_labels_multiple_idexes_medoids\"\n",
        "\n",
        "evaluate_knn_search(queries_features, queries_labels, K, PIVOTS, REORDERED_FEATURES, REORDERED_LABELS, from_zip=True, query_perturbation=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckeOEqWOuIEn"
      },
      "source": [
        "##Search for Best Combination of Pivot Number and Prefix Lenght\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9ejs5fvGsS8"
      },
      "source": [
        "def search_candidates(query_features, prefix_tree, pivots, Z, features_file, labels_file,PREFIX_LEN):\n",
        "\n",
        "  query_prefix = compute_permutation_prefix(query_features, PREFIX_LEN, pivots)\n",
        "  subtree = PrefixTree.find_prefix(prefix_tree, query_prefix, Z)\n",
        "  PrefixTree.print_tree(subtree)\n",
        "  leaves = PrefixTree.get_leaves(subtree)\n",
        "  df_features = pd.read_csv(os.path.join(BASE_PP_INDEX_PATH,features_file), sep=',', header=None, skiprows= leaves[0], nrows=(leaves[-1] - leaves[0] +1)).to_numpy()\n",
        "  df_labels = pd.read_csv(os.path.join(BASE_PP_INDEX_PATH,labels_file), sep=',', header=None, skiprows= leaves[0], nrows=(leaves[-1] - leaves[0] +1)).to_numpy().reshape(-1)\n",
        "\n",
        "  return df_features, df_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtv4gl8azBCm"
      },
      "source": [
        "def search(queries_features, K, PREFIX_TREE, N_CANDIDATES, PIVOTS, REORDERED_FEATURES, REORDERED_LABELS, PREVIEW_SIZE, PREFIX_LEN, from_zip = True, attachID = False):\n",
        "  '''\n",
        "  Search for k best images similar to queries using PP-Index\n",
        "  Returns topk_images_list, topk_scores_list, topk_labels_list\n",
        "  '''\n",
        "\n",
        "  topk_images_list = []\n",
        "  topk_scores_list = []\n",
        "  topk_labels_list = []\n",
        "  n_candidates = []\n",
        "\n",
        "  for query in tqdm(queries_features):\n",
        "    candidates, candidates_labels = search_candidates(query, PREFIX_TREE, PIVOTS, N_CANDIDATES, REORDERED_FEATURES, REORDERED_LABELS,PREFIX_LEN)\n",
        "    topk_paths,  topk_scores, topk_labels= k_nn_search(query, candidates, candidates_labels, K, from_zip, attachID)\n",
        "    n_candidates.append(len(candidates))\n",
        "\n",
        "    if not attachID:\n",
        "      topk_images = [img_to_array(load_img(path, target_size=(PREVIEW_SIZE,PREVIEW_SIZE))) for path in  topk_paths]\n",
        "      topk_images_list.append(topk_images)\n",
        "    else:\n",
        "      topk_images_list.append(topk_paths)\n",
        "      \n",
        "    topk_scores_list.append(topk_scores)\n",
        "    topk_labels_list.append(topk_labels)\n",
        "\n",
        "  return topk_images_list, topk_scores_list, topk_labels_list, n_candidates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inEGNV1PyL86"
      },
      "source": [
        "def evaluate_knn_search_return_ap(queries_features, queries_labels, k, PREFIX_TREE, N_CANDIDATES, PIVOTS, REORDERED_FEATURES, REORDERED_LABELS, PREVIEW_SIZE,PREFIX_LEN, from_zip=True):\n",
        "  '''\n",
        "  Evaluate APs and mean AP of all queries\n",
        "  '''\n",
        "\n",
        "  # Search \n",
        "  topk_images, topk_scores, topk_labels,n_candidates = search(queries_features, k, PREFIX_TREE, N_CANDIDATES, PIVOTS, REORDERED_FEATURES, REORDERED_LABELS, PREVIEW_SIZE,PREFIX_LEN, from_zip)\n",
        "  # If a certain retreived image is relevant \n",
        "  topk_is_relevant = np.array(topk_labels) == queries_labels.reshape(-1,1)\n",
        "\n",
        "  # Average Precision Score computation\n",
        "  aps = np.array([sklearn.metrics.average_precision_score(l, s) for l,s in zip(topk_is_relevant, topk_scores)])\n",
        "  aps[np.isnan(aps)] = 0\n",
        "  print('APs per Query:', aps)\n",
        "  print('mAP:', np.mean(aps))\n",
        "\n",
        "  return np.mean(aps), n_candidates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pHSx4novlm9"
      },
      "source": [
        "#map, recall, number of candidates\n",
        "\n",
        "BASE_PP_INDEX_PATH = \"/content/gdrive/My Drive/Data/PP-Index\"\n",
        "FEATURES_PATH = \"/content/gdrive/My Drive/Data/features\"\n",
        "CHUNKSIZE = 1000\n",
        "\n",
        "def get_map_candidates(pivots):\n",
        "\n",
        "  global_map = []\n",
        "  global_n_candidates = []\n",
        "\n",
        "  for pivot in pivots:\n",
        "\n",
        "    pivot_map = []\n",
        "    pivot_candidates = []\n",
        "\n",
        "    PREFIX_LEN = list(range(2,pivot+1))\n",
        "\n",
        "    extract_random_pivots('finetuned_features.csv','optimized_pivots.csv', pivot)\n",
        "    #extract_k_medoids_pivots('finetuned_features.csv', 'optimized_pivots.csv', 25000, pivot)\n",
        "\n",
        "    # Dataset of images permutations of indexes \n",
        "    create_permutation_dataset(\"finetuned_features.csv\", \"optimized_permutations.csv\", \"optimized_pivots.csv\", CHUNKSIZE)\n",
        "\n",
        "    for prefix in PREFIX_LEN:\n",
        "\n",
        "      # Creates Empty Prefix Tree\n",
        "      prefix_tree_root = PrefixTree(\"Prefix Tree Optimized\"+str(pivot)+\"-\"+str(prefix))\n",
        "      # Adds all images pivots permutations in \"permutations.csv\"\n",
        "      PrefixTree.add_from_csv(prefix_tree_root, \"optimized_permutations.csv\", prefix, chunksize = 1000)\n",
        "\n",
        "      # Get images indexes ordered by the prefix tree\n",
        "      indexes_reordered_by_prefix_tree = PrefixTree.get_leaves(prefix_tree_root)\n",
        "      print(indexes_reordered_by_prefix_tree)\n",
        "\n",
        "      # Reorder indexes to obtain a mapping necessary to quick memory sort\n",
        "      indexing_mapping = [indexes_reordered_by_prefix_tree.index(i) for i in tqdm(range(len(indexes_reordered_by_prefix_tree)))]\n",
        "      np.save(os.path.join(BASE_PP_INDEX_PATH,\"indexing_mapping_opt\"), indexing_mapping)\n",
        "      print(indexing_mapping)\n",
        "\n",
        "      # In memory features and labels reordering\n",
        "      reorder_datastore_in_memory('finetuned_features.csv', 'reordered_optimized_finetuned_features.csv', indexing_mapping)\n",
        "      reorder_datastore_in_memory('labels.csv', 'reordered_optimized_finetuned_labels.csv', indexing_mapping)\n",
        "\n",
        "      # Merge leaves in order to have only start and end index for a certain permutation\n",
        "      PrefixTree.merge_leaves(prefix_tree_root)\n",
        "      PrefixTree.print_tree(prefix_tree_root)\n",
        "\n",
        "      N_QUERIES = 10\n",
        "      K = 100\n",
        "      N_CANDIDATES = 300\n",
        "      PREFIX_TREE = prefix_tree_root\n",
        "      PIVOTS = np.array(pd.read_csv(os.path.join(BASE_PP_INDEX_PATH,\"optimized_pivots.csv\"), sep=',',header=None))\n",
        "      REORDERED_FEATURES = \"reordered_optimized_finetuned_features.csv\"\n",
        "      REORDERED_LABELS = \"reordered_optimized_finetuned_labels.csv\"\n",
        "      PREVIEW_SIZE = 100\n",
        "\n",
        "      #sarà private test dataset\n",
        "      test_features = np.array(pd.read_csv(os.path.join(FEATURES_PATH, \"finetuned_features_private.csv\"), sep=',',header=None))\n",
        "      test_Labels = np.array(pd.read_csv(os.path.join(FEATURES_PATH, \"private_test_labels.csv\"), sep=',',header=None)).reshape(-1)\n",
        "\n",
        "      # query_indexes = [10, 11]\n",
        "      # query_indexes = [14, 78, 79, 12, 18]\n",
        "      query_indexes = [100, 500,  700, 1200,  2200, 2900, 3400]\n",
        "      #query_indexes = np.random.choice(len(test_features), N_QUERIES, replace=False)  # get n queries at random\n",
        "      queries_features = test_features[query_indexes]\n",
        "      queries_labels = test_Labels[query_indexes]\n",
        "\n",
        "      map, n_candidates = evaluate_knn_search_return_ap(queries_features, queries_labels, K, PREFIX_TREE, N_CANDIDATES, PIVOTS, REORDERED_FEATURES, REORDERED_LABELS, PREVIEW_SIZE,prefix)\n",
        "      pivot_map.append(map)\n",
        "      pivot_candidates.append(n_candidates)\n",
        "\n",
        "    global_map.append(pivot_map)\n",
        "    global_n_candidates.append(pivot_candidates)\n",
        "\n",
        "\n",
        "  return global_map, global_n_candidates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ievl3fxI1yTR"
      },
      "source": [
        "map, candidates = get_map_candidates([5,6,7,8,9,10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCr6l6EmfF3k"
      },
      "source": [
        "###Random Pivot Selection Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isxayVlDfF3l"
      },
      "source": [
        "for elem in map:\n",
        "  print(elem)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmpGQxdofF3l"
      },
      "source": [
        "for elem in candidates:\n",
        "  elem =np.array(elem)\n",
        "  print(np.mean(elem,axis=1).round())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWEnENoYPjo6"
      },
      "source": [
        "###K-Medoids Pivot Selection Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzU-sgNMPjo6"
      },
      "source": [
        "for elem in map:\n",
        "  print(elem)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLpSg9-6Pjo7"
      },
      "source": [
        "for elem in candidates:\n",
        "  elem =np.array(elem)\n",
        "  print(np.mean(elem,axis=1).round())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aofpQKBkjsSj"
      },
      "source": [
        "###Results Evaluation\n",
        "In order to compare the results, a summary score is derived from MAP and MCANDIDATES. This score is used to make a quick comparision, and it has not definitive importance. In particular:\n",
        "\n",
        "\\begin{equation}\n",
        "score= weight_{map}*MAP + weight_{mcandidates}*( 1 - normalized\\_MCANDIDATES )\n",
        "\\end{equation}\n",
        "\n",
        "where: <br>\n",
        "$MAP$: Mean Average Precision for a given configuration <br>\n",
        "$MCANDIDATES$: Mean Number of Candidates considered <br>\n",
        "$normalized\\_MCANDIDATES$: min-max normalization of MCANDIDATES assuming 300 as minimum value (300 is the minimum number of candidates) and 57298 as as maximum value (cardinality of the whole dataset, private test set excluded). <br>\n",
        "$weight_{map}$: fixed to $\\frac{2}{3}$ <br>\n",
        "$weight_{mcandidates}$: fixed $\\frac{1}{3}$ <br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrLQNABjvkW1"
      },
      "source": [
        "\"\"\"\n",
        "CREATION OF THE ARRAY CONTAINING PIVOT-PREFIX PAIRS\n",
        "Number of pivots varies between 5 and 10, while prefix varies from 2\n",
        "to the number of pivots considered. \n",
        "\"\"\"\n",
        "\n",
        "pivots = np.arange(5,11)\n",
        "pivot_prefix_array = []\n",
        "\n",
        "for pivot in pivots:\n",
        "\n",
        "  pivot_array = []\n",
        "\n",
        "  for prefix in np.arange(2,pivot+1):\n",
        "\n",
        "    pivot_array.append([pivot, prefix])\n",
        "\n",
        "  pivot_prefix_array.append(pivot_array)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIc3hVeJs3D5"
      },
      "source": [
        "\"\"\"\n",
        "SCORES COMPUTATION FOR K-MEDOIDS-PIVOT-SELECTION EXPERIMENT\n",
        "Scores for K-Medoids experiments are computed according to the formula described above.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "maps2 = np.array([ [0.42797970216219827, 0.4473705587136627, 0.4405917904811597, 0.4405917904811597],\n",
        "                   [0.39857218910802683, 0.3938975786910332, 0.3893925133982128, 0.38617527397027535, 0.38617527397027535],\n",
        "                   [0.4222522247273362, 0.42691657055673315, 0.45032691652410195, 0.41592846774276016, 0.40809568227905463, 0.40809568227905463],\n",
        "                   [0.3435707089773567, 0.3616436592036507, 0.3637343130382096, 0.36100876507821483, 0.35546469664712027, 0.35546469664712027, 0.35546469664712027],\n",
        "                   [0.35632062471054693, 0.36425998668564935, 0.35709273083158255, 0.36129285725856247, 0.36636986992321263, 0.3650510393241892, 0.3649330819984303, 0.3649330819984303],\n",
        "                   [0.4223070580277098, 0.4204660304954248, 0.41989962124115127, 0.4209338620053527, 0.4209338620053527, 0.4398023948565452, 0.4398023948565452, 0.4398023948565452, 0.4398023948565452] ], dtype=object)\n",
        "\n",
        "mcandidates2 = np.array([ [5087, 3454, 2371, 2371],\n",
        "                          [5681, 4923, 4399, 3736, 3736],\n",
        "                          [2864, 2112, 1568, 1309, 1175, 1175],\n",
        "                          [2998, 2264, 2098, 2073, 1974, 1970, 1970],\n",
        "                          [3595, 2942, 2738, 2495, 2457, 2417, 2414, 2414],\n",
        "                          [3821, 2827, 2528, 2482, 2476, 2473, 2473, 2473, 2473] ],dtype=object)\n",
        "\n",
        "#300 is the minimum number of candidates considered, while 57298 is the cardinality of the whole dataset\n",
        "min=300\n",
        "max=57298\n",
        "\n",
        "map_weight = 2/3\n",
        "mcandidate_weight = 1/3\n",
        "\n",
        "mcandidates2_normalized = copy.deepcopy(mcandidates2)\n",
        "\n",
        "#min-max normalization of MCANDIDATES using 300 as min value and 57298 as max value\n",
        "for i in np.arange(pivots.size):\n",
        "\n",
        "  for j in np.arange(len(mcandidates2[i])):\n",
        "\n",
        "    mcandidates2_normalized[i][j] = (mcandidates2_normalized[i][j]-min)/(max-min)\n",
        "\n",
        "#scores computation according to the formula described above\n",
        "scores2 = []\n",
        "\n",
        "for i in np.arange(pivots.size):\n",
        "\n",
        "  pivot_scores = []\n",
        "\n",
        "  for j in np.arange(len(mcandidates2_normalized[i])):\n",
        "\n",
        "    map = maps2[i][j]\n",
        "    mcandidate_normalized = mcandidates2_normalized[i][j]\n",
        "    score = (map_weight*map+mcandidate_weight*(1-mcandidate_normalized))\n",
        "    pivot_scores.append(score)\n",
        "\n",
        "  scores2.append(pivot_scores)\n",
        "\n",
        "#print the scores\n",
        "for i in np.arange(pivots.size):\n",
        "\n",
        "  for j in np.arange(len(scores2[i])):\n",
        "\n",
        "    print(\"PIVOTS: \" + str(pivot_prefix_array[i][j][0]) + \" PREFIX: \"+ str(pivot_prefix_array[i][j][1]) + \"\\t SCORE: \" + str(np.round(scores2[i][j],5)) + \n",
        "          \"\\t CANDIDATES: \" + str(mcandidates2[i][j]) + \"\\t NORMALIZED CANDIDATES: \" + str(np.round(mcandidates2_normalized[i][j],5)) + \"\\t MAP: \" + str(np.round(maps2[i][j],5)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSNtp1Hus6Lx"
      },
      "source": [
        "\"\"\"\n",
        "SCORES COMPUTATION FOR RANDOM-PIVOT-SELECTION EXPERIMENT\n",
        "Scores for Random pivots extraction experiments are computed according to the formula described above.\n",
        "\"\"\"\n",
        "\n",
        "maps3 = np.array( [ [0.5159362375487182, 0.4209606316965547, 0.3879634867332293, 0.3879634867332293],\n",
        "                    [0.46054902825532285, 0.36566337362714, 0.37835149760185427, 0.3502531769493248, 0.3502531769493248],\n",
        "                    [0.5002595477724913, 0.4640710175155756, 0.46451728465019315, 0.4662482028715962, 0.46547039141841834, 0.46547039141841834],\n",
        "                    [0.42187718416723957, 0.43694466854882236, 0.4228089425608541, 0.4257841391972833, 0.4257796505718903, 0.4257796505718903, 0.4257796505718903],\n",
        "                    [0.48502996209122184, 0.4872694411078049, 0.4780190972542616, 0.46919987803258933, 0.4645432222904482, 0.4645432222904482, 0.46515521596576204, 0.46515521596576204],\n",
        "                    [0.44521338037565844, 0.4681250592452478, 0.5074289379589547, 0.5074289379589547, 0.5074289379589547, 0.5074289379589547, 0.5074289379589547, 0.5074289379589547, 0.5074289379589547] ], dtype=object)\n",
        "\n",
        "mcandidates3= np.array([ [6033, 4256, 3766, 3766],\n",
        "                   [6146, 1793, 1010,  626,  626],\n",
        "                   [4270, 2194, 1980, 1711, 1698, 1698],\n",
        "                   [1874, 1137,  775,  595,  595,  595,  595],\n",
        "                   [4724, 2135, 1637, 1536, 1480, 1477, 1475, 1475],\n",
        "                   [2550, 1874, 1663, 1663, 1663, 1663, 1663, 1663, 1663] ], dtype=object)\n",
        "\n",
        "#300 is the minimum number of candidates considered, while 57298 is the cardinality of the whole dataset\n",
        "max = 57298\n",
        "min = 300\n",
        "\n",
        "map_weight = 2/3\n",
        "mcandidate_weight = 1/3\n",
        "\n",
        "mcandidates3_normalized = copy.deepcopy(mcandidates3)\n",
        "\n",
        "#min-max normalization of MCANDIDATES using 300 as min value and 57298 as max value\n",
        "for i in np.arange(pivots.size):\n",
        "\n",
        "  for j in np.arange(len(mcandidates3[i])):\n",
        "\n",
        "    mcandidates3_normalized[i][j] = (mcandidates3_normalized[i][j]-min)/(max-min)\n",
        "\n",
        "scores3 = []\n",
        "\n",
        "#scores computation according to the formula described above\n",
        "for i in np.arange(pivots.size):\n",
        "\n",
        "  pivot_scores = []\n",
        "\n",
        "  for j in np.arange(len(mcandidates3_normalized[i])):\n",
        "\n",
        "    map = maps3[i][j]\n",
        "    mcandidates_normalized = mcandidates3_normalized[i][j]\n",
        "    score = (map_weight*map+mcandidate_weight*(1-mcandidates_normalized))\n",
        "    pivot_scores.append(score)\n",
        "\n",
        "  scores3.append(pivot_scores)\n",
        "\n",
        "#print the scores\n",
        "for i in np.arange(pivots.size):\n",
        "\n",
        "  for j in np.arange(len(scores3[i])):\n",
        "\n",
        "    print(\"PIVOTS: \" + str(pivot_prefix_array[i][j][0]) + \" PREFIX: \"+ str(pivot_prefix_array[i][j][1]) + \"\\t SCORE: \" + str(np.round(scores3[i][j],5)) + \n",
        "          \"\\t CANDIDATES: \" + str(mcandidates3[i][j]) + \"\\t NORMALIZED CANDIDATES: \" + str(np.round(mcandidates3_normalized[i][j],5)) + \"\\t MAP: \" + str(np.round(maps3[i][j],5)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}